# GPT-2ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆé–‹ç™ºã‚¬ã‚¤ãƒ‰ - Google Colabç‰ˆï¼ˆæ”¹è‰¯ç‰ˆï¼‰

ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€Hugging Faceã®GPT-2äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦Google Colabã§å®Ÿç”¨çš„ãªãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã‚’ä½œæˆã™ã‚‹æ–¹æ³•ã‚’è©³ã—ãèª¬æ˜ã—ã¾ã™ã€‚åˆå¿ƒè€…ã®æ–¹ã§ã‚‚ç†è§£ã§ãã‚‹ã‚ˆã†ã€å„ã‚¹ãƒ†ãƒƒãƒ—ã‚’ä¸å¯§ã«è§£èª¬ã—ã€å®Ÿéš›ã«å‹•ä½œã™ã‚‹ã‚³ãƒ¼ãƒ‰ã‚’æä¾›ã—ã¾ã™ã€‚

## 1. ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã¨ç†è§£

### Google Colabã®è¨­å®š

ã¾ãšã€Google Colabã§GPUã‚’æœ‰åŠ¹åŒ–ã—ã¾ã—ã‚‡ã†ã€‚ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã®ã€Œãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã€â†’ã€Œãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã®ã‚¿ã‚¤ãƒ—ã‚’å¤‰æ›´ã€â†’ã€Œãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã‚¢ã‚¯ã‚»ãƒ©ãƒ¬ãƒ¼ã‚¿ã€ã‚’ã€ŒGPUã€ã«è¨­å®šã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã¨æ¨è«–ãŒå¤§å¹…ã«é«˜é€ŸåŒ–ã•ã‚Œã¾ã™ã€‚

### å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®è©³ç´°èª¬æ˜

å„ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®å½¹å‰²ã‚’ç†è§£ã™ã‚‹ã“ã¨ã§ã€ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸéš›ã®å¯¾å‡¦æ³•ã‚‚åˆ†ã‹ã‚Šã‚„ã™ããªã‚Šã¾ã™ã€‚

- **transformers**: Hugging Faceã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã€GPT-2ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’æä¾›
- **torch**: PyTorchã®æ·±å±¤å­¦ç¿’ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
- **datasets**: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®åŠ¹ç‡çš„ãªå‡¦ç†ã‚’è¡Œã†
- **accelerate**: åˆ†æ•£å­¦ç¿’ã¨GPUä½¿ç”¨ã®æœ€é©åŒ–

```python
# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
# ã“ã®ãƒ—ãƒ­ã‚»ã‚¹ã«ã¯æ•°åˆ†ã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™
!pip install transformers==4.21.0 torch==1.12.1 datasets==2.4.0 accelerate==0.12.0

# æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ç”¨ã®è¿½åŠ ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
!pip install sentencepiece==0.1.97 sacremoses==0.0.53

print("ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
```

### åŸºæœ¬ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¨è¨­å®š

å„ã‚¤ãƒ³ãƒãƒ¼ãƒˆã®æ„å‘³ã¨å¿…è¦æ€§ã‚’èª¬æ˜ã—ã¾ã™ã€‚

```python
import torch
from transformers import (
    GPT2LMHeadModel, 
    GPT2Tokenizer, 
    TrainingArguments, 
    Trainer,
    DataCollatorForLanguageModeling
)
from datasets import Dataset
import json
import re
import random
from typing import List, Dict, Optional
import warnings
from datetime import datetime
import os

# è­¦å‘Šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’éè¡¨ç¤ºã«ã™ã‚‹ï¼ˆé–‹ç™ºæ™‚ã®é›‘éŸ³ã‚’æ¸›ã‚‰ã™ãŸã‚ï¼‰
warnings.filterwarnings('ignore')

# ãƒ‡ãƒã‚¤ã‚¹è¨­å®šã®è©³ç´°ç¢ºèª
if torch.cuda.is_available():
    device = torch.device('cuda')
    print(f"GPUä½¿ç”¨: {torch.cuda.get_device_name(0)}")
    print(f"åˆ©ç”¨å¯èƒ½VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
else:
    device = torch.device('cpu')
    print("CPUä½¿ç”¨ï¼ˆè­¦å‘Š: å­¦ç¿’ã«æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ï¼‰")

# å†ç¾æ€§ã®ãŸã‚ã®ä¹±æ•°ã‚·ãƒ¼ãƒ‰è¨­å®š
torch.manual_seed(42)
random.seed(42)
```

## 2. ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®è©³ç´°è¨­å®š

### GPT-2ãƒ¢ãƒ‡ãƒ«ã®é¸æŠã¨ç‰¹å¾´

GPT-2ã«ã¯è¤‡æ•°ã®ã‚µã‚¤ã‚ºãŒã‚ã‚Šã¾ã™ï¼š
- **gpt2** (124M ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿): æœ€å°ã‚µã‚¤ã‚ºã€Colabã§å®‰å®šå‹•ä½œ
- **gpt2-medium** (355M): ã‚ˆã‚Šé«˜å“è³ªã ãŒãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡å¤§
- **gpt2-large** (774M): ã•ã‚‰ã«é«˜å“è³ªã ãŒColabç„¡æ–™ç‰ˆã§ã¯å³ã—ã„

```python
# ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã®é¸æŠï¼ˆãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’è€ƒæ…®ï¼‰
model_name = "gpt2"  # Colabç„¡æ–™ç‰ˆã«æœ€é©

print(f"ãƒ¢ãƒ‡ãƒ« '{model_name}' ã‚’èª­ã¿è¾¼ã¿ä¸­...")

try:
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿
    tokenizer = GPT2Tokenizer.from_pretrained(model_name)
    
    # GPT-2ã¯å…ƒã€…ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ãŒãªã„ãŸã‚ã€EOSãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä½¿ç”¨
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        print("ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ã‚’EOSãƒˆãƒ¼ã‚¯ãƒ³ã«è¨­å®šã—ã¾ã—ãŸã€‚")
    
    # ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
    model = GPT2LMHeadModel.from_pretrained(model_name)
    
    # GPUã¸ã®ç§»å‹•
    model.to(device)
    
    # ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã®è¡¨ç¤º
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print(f"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†!")
    print(f"ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {total_params:,}")
    print(f"å­¦ç¿’å¯¾è±¡ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {trainable_params:,}")
    
except Exception as e:
    print(f"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    print("ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶šã¾ãŸã¯Hugging Faceã®ã‚µãƒ¼ãƒãƒ¼çŠ¶æ³ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
```

## 3. é«˜å“è³ªãªä¼šè©±ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ§‹ç¯‰

### ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­è¨ˆã®é‡è¦æ€§

ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã®å“è³ªã¯å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«å¤§ããä¾å­˜ã—ã¾ã™ã€‚å¤šæ§˜ã§è‡ªç„¶ãªä¼šè©±ä¾‹ã‚’ç”¨æ„ã™ã‚‹ã“ã¨ãŒé‡è¦ã§ã™ã€‚

```python
# é«˜å“è³ªãªæ—¥æœ¬èªä¼šè©±ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
def create_comprehensive_dataset():
    """
    å¤šæ§˜ãªã‚·ãƒŠãƒªã‚ªã‚’ã‚«ãƒãƒ¼ã™ã‚‹ä¼šè©±ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ
    å„ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‹ã‚‰è‡ªç„¶ãªä¼šè©±ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åé›†
    """
    
    conversations = [
        # æŒ¨æ‹¶ãƒ»åŸºæœ¬ä¼šè©±
        {"user": "ãŠã¯ã‚ˆã†ã”ã–ã„ã¾ã™", "bot": "ãŠã¯ã‚ˆã†ã”ã–ã„ã¾ã™ï¼ä»Šæ—¥ã‚‚è‰¯ã„ä¸€æ—¥ã‚’ãŠéã”ã—ãã ã•ã„ã€‚ä½•ã‹ãŠæ‰‹ä¼ã„ã§ãã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ"},
        {"user": "ã“ã‚“ã«ã¡ã¯", "bot": "ã“ã‚“ã«ã¡ã¯ï¼ãŠç–²ã‚Œã•ã¾ã§ã™ã€‚ä»Šæ—¥ã¯ã©ã®ã‚ˆã†ãªã“ã¨ã«ã¤ã„ã¦ãŠè©±ã—ã¾ã—ã‚‡ã†ã‹ï¼Ÿ"},
        {"user": "ã“ã‚“ã°ã‚“ã¯", "bot": "ã“ã‚“ã°ã‚“ã¯ï¼ä¸€æ—¥ãŠç–²ã‚Œã•ã¾ã§ã—ãŸã€‚ãƒªãƒ©ãƒƒã‚¯ã‚¹ã—ã¦ãŠè©±ã—ã¾ã—ã‚‡ã†ã€‚"},
        
        # æ„Ÿæƒ…ãƒ»çŠ¶æ…‹ç¢ºèª
        {"user": "å…ƒæ°—ã§ã™ã‹ï¼Ÿ", "bot": "ã¯ã„ã€ã¨ã¦ã‚‚å…ƒæ°—ã§ã™ï¼ã‚ãªãŸã¯ã„ã‹ãŒã§ã™ã‹ï¼Ÿä»Šæ—¥ã¯èª¿å­ã¯ã©ã†ã§ã—ã‚‡ã†ï¼Ÿ"},
        {"user": "ç–²ã‚Œã¾ã—ãŸ", "bot": "ãŠç–²ã‚Œã•ã¾ã§ã™ã€‚å°‘ã—ä¼‘æ†©ã‚’å–ã‚‰ã‚Œã¦ã¯ã„ã‹ãŒã§ã—ã‚‡ã†ã‹ï¼Ÿä½•ã‹æ°—åˆ†è»¢æ›ã«ãªã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ"},
        {"user": "å¬‰ã—ã„ã“ã¨ãŒã‚ã‚Šã¾ã—ãŸ", "bot": "ãã‚Œã¯ç´ æ™´ã‚‰ã—ã„ã§ã™ã­ï¼ã‚ˆã‚ã—ã‘ã‚Œã°ã€ã©ã®ã‚ˆã†ãªå¬‰ã—ã„ã“ã¨ãŒã‚ã£ãŸã®ã‹æ•™ãˆã¦ãã ã•ã„ã€‚"},
        
        # æ—¥å¸¸ä¼šè©±
        {"user": "ä»Šæ—¥ã¯ä½•ã‚’ã—ã¦ã„ã¾ã—ãŸã‹ï¼Ÿ", "bot": "ç§ã¯AIãªã®ã§ç‰¹åˆ¥ãªæ´»å‹•ã¯ã—ã¦ã„ã¾ã›ã‚“ãŒã€å¤šãã®æ–¹ã¨ãŠè©±ã—ã™ã‚‹ã“ã¨ãŒã§ãã¦å……å®Ÿã—ã¦ã„ã¾ã™ã€‚ã‚ãªãŸã¯ä»Šæ—¥ã©ã®ã‚ˆã†ãªä¸€æ—¥ã§ã—ãŸã‹ï¼Ÿ"},
        {"user": "å¤©æ°—ã¯ã©ã†ã§ã™ã‹ï¼Ÿ", "bot": "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã®å¤©æ°—æƒ…å ±ã¯å–å¾—ã§ãã¾ã›ã‚“ã€‚ãŠä½ã¾ã„ã®åœ°åŸŸã®å¤©æ°—äºˆå ±ã‚¢ãƒ—ãƒªã‚„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’ã”ç¢ºèªãã ã•ã„ã€‚"},
        {"user": "æ™‚é–“ã‚’æ•™ãˆã¦", "bot": "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€ç¾åœ¨æ™‚åˆ»ã¯è¡¨ç¤ºã§ãã¾ã›ã‚“ã€‚ãƒ‡ãƒã‚¤ã‚¹ã®æ™‚è¨ˆã‚’ã”ç¢ºèªãã ã•ã„ã€‚"},
        
        # è¶£å‘³ãƒ»å¨¯æ¥½
        {"user": "ãŠã™ã™ã‚ã®æ˜ ç”»ã‚’æ•™ãˆã¦", "bot": "æ˜ ç”»ã®ã‚¸ãƒ£ãƒ³ãƒ«ã«ã‚ˆã‚Šã¾ã™ã­ã€‚ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã€ãƒ‰ãƒ©ãƒã€ã‚³ãƒ¡ãƒ‡ã‚£ã€ã‚¢ãƒ‹ãƒ¡ãªã©ã€ã©ã¡ã‚‰ãŒãŠå¥½ã¿ã§ã—ã‚‡ã†ã‹ï¼Ÿå¥½ã¿ã‚’æ•™ãˆã¦ã„ãŸã ã‘ã‚Œã°ã€ã‚ˆã‚Šå…·ä½“çš„ã«ãŠç­”ãˆã§ãã¾ã™ã€‚"},
        {"user": "éŸ³æ¥½ã¯å¥½ãã§ã™ã‹ï¼Ÿ", "bot": "éŸ³æ¥½ã«ã¤ã„ã¦è©±ã™ã®ã¯æ¥½ã—ã„ã§ã™ï¼ã‚¯ãƒ©ã‚·ãƒƒã‚¯ã€ãƒãƒƒãƒ—ã€ãƒ­ãƒƒã‚¯ã€ã‚¸ãƒ£ã‚ºãªã©æ§˜ã€…ãªã‚¸ãƒ£ãƒ³ãƒ«ãŒã‚ã‚Šã¾ã™ã­ã€‚ã©ã®ã‚ˆã†ãªéŸ³æ¥½ãŒãŠå¥½ãã§ã™ã‹ï¼Ÿ"},
        {"user": "èª­æ›¸ã«ã¤ã„ã¦", "bot": "èª­æ›¸ã¯çŸ¥è­˜ã‚’åºƒã’ã‚‹ç´ æ™´ã‚‰ã—ã„è¶£å‘³ã§ã™ã­ã€‚å°èª¬ã€ãƒ“ã‚¸ãƒã‚¹æ›¸ã€è‡ªå·±å•“ç™ºæ›¸ãªã©ã€ã©ã®ã‚ˆã†ãªã‚¸ãƒ£ãƒ³ãƒ«ã®æœ¬ã‚’èª­ã¾ã‚Œã¾ã™ã‹ï¼Ÿ"},
        
        # å­¦ç¿’ãƒ»ä»•äº‹
        {"user": "å‹‰å¼·ã«ã¤ã„ã¦ç›¸è«‡ãŒã‚ã‚Šã¾ã™", "bot": "å‹‰å¼·ã«ã¤ã„ã¦ãŠæ‰‹ä¼ã„ã§ãã‚‹ã“ã¨ãŒã‚ã‚Œã°å¬‰ã—ã„ã§ã™ã€‚ã©ã®ã‚ˆã†ãªåˆ†é‡ã®å­¦ç¿’ã§ã—ã‚‡ã†ã‹ï¼ŸåŠ¹ç‡çš„ãªå­¦ç¿’æ–¹æ³•ã«ã¤ã„ã¦ã‚‚ãŠè©±ã—ã§ãã¾ã™ã‚ˆã€‚"},
        {"user": "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã‚’å­¦ã³ãŸã„", "bot": "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°å­¦ç¿’ã¯ç´ æ™´ã‚‰ã—ã„æŒ‘æˆ¦ã§ã™ã­ï¼Pythonã€JavaScriptã€Javaãªã©ã€ã©ã®è¨€èªã«èˆˆå‘³ãŒã‚ã‚Šã¾ã™ã‹ï¼Ÿåˆå¿ƒè€…å‘ã‘ã®ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚‚ã§ãã¾ã™ã€‚"},
        {"user": "ä»•äº‹ãŒå¤§å¤‰ã§ã™", "bot": "ä»•äº‹ã§ãŠç–²ã‚Œã®ã‚ˆã†ã§ã™ã­ã€‚å…·ä½“çš„ã«ã©ã®ã‚ˆã†ãªç‚¹ãŒå¤§å¤‰ã§ã—ã‚‡ã†ã‹ï¼Ÿæ™‚é–“ç®¡ç†ã‚„åŠ¹ç‡åŒ–ã«ã¤ã„ã¦ä¸€ç·’ã«è€ƒãˆã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã‚ˆã€‚"},
        
        # æ„Ÿè¬ãƒ»çµ‚äº†
        {"user": "ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™", "bot": "ã©ã†ã„ãŸã—ã¾ã—ã¦ï¼ãŠå½¹ã«ç«‹ã¦ã¦å¬‰ã—ã„ã§ã™ã€‚ä»–ã«ã‚‚ä½•ã‹ã”è³ªå•ã‚„ãŠè©±ã—ã—ãŸã„ã“ã¨ãŒã‚ã‚Œã°ã€ã„ã¤ã§ã‚‚ãŠæ°—è»½ã«ã©ã†ãã€‚"},
        {"user": "ã¾ãŸä»Šåº¦è©±ã—ã¾ã—ã‚‡ã†", "bot": "ã¯ã„ã€ãœã²ã¾ãŸãŠè©±ã—ã¾ã—ã‚‡ã†ï¼ã„ã¤ã§ã‚‚ãŠå¾…ã¡ã—ã¦ã„ã¾ã™ã€‚ç´ æ™´ã‚‰ã—ã„æ™‚é–“ã‚’ãŠéã”ã—ãã ã•ã„ã€‚"},
        {"user": "ã•ã‚ˆã†ãªã‚‰", "bot": "ã•ã‚ˆã†ãªã‚‰ï¼ä»Šæ—¥ã¯ãŠè©±ã—ã§ãã¦æ¥½ã—ã‹ã£ãŸã§ã™ã€‚ã¾ãŸæ¬¡å›ãŠä¼šã„ã§ãã‚‹ã“ã¨ã‚’æ¥½ã—ã¿ã«ã—ã¦ã„ã¾ã™ã€‚"},
        
        # ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ç”¨
        {"user": "ã‚ˆãã‚ã‹ã‚‰ãªã„è³ªå•", "bot": "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€è³ªå•ã®å†…å®¹ã‚’ã‚‚ã†å°‘ã—å…·ä½“çš„ã«æ•™ãˆã¦ã„ãŸã ã‘ã¾ã™ã‹ï¼Ÿã‚ˆã‚Šé©åˆ‡ã«ãŠç­”ãˆã§ãã‚‹ã‚ˆã†åŠªã‚ã¾ã™ã€‚"},
    ]
    
    return conversations

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
conversations = create_comprehensive_dataset()
print(f"ä¼šè©±ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆå®Œäº†: {len(conversations)}ä»¶ã®ä¼šè©±ãƒšã‚¢")

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å†…å®¹ç¢ºèª
print("\n=== ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¾‹ ===")
for i, conv in enumerate(conversations[:3]):
    print(f"ä¾‹ {i+1}:")
    print(f"  ãƒ¦ãƒ¼ã‚¶ãƒ¼: {conv['user']}")
    print(f"  ãƒœãƒƒãƒˆ: {conv['bot']}")
    print()
```

### å¤–éƒ¨ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿æ©Ÿèƒ½

```python
def load_conversation_data(file_path: str) -> List[Dict[str, str]]:
    """
    JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ä¼šè©±ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€é–¢æ•°
    
    JSONãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ä¾‹:
    [
        {"user": "è³ªå•1", "bot": "å›ç­”1"},
        {"user": "è³ªå•2", "bot": "å›ç­”2"}
    ]
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # ãƒ‡ãƒ¼ã‚¿å½¢å¼ã®æ¤œè¨¼
        if not isinstance(data, list):
            raise ValueError("ãƒ‡ãƒ¼ã‚¿ã¯é…åˆ—å½¢å¼ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™")
        
        for item in data:
            if not isinstance(item, dict) or 'user' not in item or 'bot' not in item:
                raise ValueError("å„è¦ç´ ã«ã¯userã¨botã‚­ãƒ¼ãŒå¿…è¦ã§ã™")
        
        print(f"å¤–éƒ¨ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ {len(data)} ä»¶ã®ä¼šè©±ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
        return data
        
    except FileNotFoundError:
        print(f"ãƒ•ã‚¡ã‚¤ãƒ« '{file_path}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
        return create_comprehensive_dataset()
    except json.JSONDecodeError as e:
        print(f"JSONãƒ•ã‚¡ã‚¤ãƒ«ã®è§£æã‚¨ãƒ©ãƒ¼: {e}")
        return create_comprehensive_dataset()
    except Exception as e:
        print(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return create_comprehensive_dataset()

# å¤–éƒ¨ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹å ´åˆã¯èª­ã¿è¾¼ã¿ã€ãªã‘ã‚Œã°ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
conversations = load_conversation_data('chatbot_data.json')
```

## 4. ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã¨ãƒˆãƒ¼ã‚¯ãƒ³åŒ–

### é«˜åº¦ãªå‰å‡¦ç†æ©Ÿèƒ½ã®å®Ÿè£…

```python
def preprocess_conversations(conversations: List[Dict[str, str]], 
                           tokenizer, 
                           max_length: int = 256) -> Dict:
    """
    ä¼šè©±ãƒ‡ãƒ¼ã‚¿ã‚’å­¦ç¿’ç”¨ã«å‰å‡¦ç†ã™ã‚‹é«˜åº¦ãªé–¢æ•°
    
    Args:
        conversations: ä¼šè©±ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
        tokenizer: GPT-2ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
        max_length: æœ€å¤§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·
    
    Returns:
        ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
    """
    
    processed_texts = []
    valid_conversations = 0
    
    print("ä¼šè©±ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ã‚’é–‹å§‹...")
    
    for i, conv in enumerate(conversations):
        try:
            # ãƒ†ã‚­ã‚¹ãƒˆã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
            user_text = conv['user'].strip()
            bot_text = conv['bot'].strip()
            
            # ç©ºæ–‡å­—ãƒã‚§ãƒƒã‚¯
            if not user_text or not bot_text:
                continue
            
            # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä½¿ç”¨ã—ãŸä¼šè©±ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
            # <|user|>ã¨<|bot|>ã§æ˜ç¢ºã«åŒºåˆ¥
            formatted_text = f"<|user|>{user_text}<|bot|>{bot_text}<|endoftext|>"
            
            # é•·ã™ãã‚‹ãƒ†ã‚­ã‚¹ãƒˆã®äº‹å‰ãƒã‚§ãƒƒã‚¯
            if len(formatted_text) > max_length * 4:  # å¤§ã¾ã‹ãªæ–‡å­—æ•°ãƒã‚§ãƒƒã‚¯
                print(f"è­¦å‘Š: ä¼šè©± {i+1} ãŒé•·ã™ãã‚‹ãŸã‚çœç•¥ã•ã‚Œã¾ã—ãŸ")
                continue
            
            processed_texts.append(formatted_text)
            valid_conversations += 1
            
        except Exception as e:
            print(f"ä¼šè©± {i+1} ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            continue
    
    print(f"å‰å‡¦ç†å®Œäº†: {valid_conversations}/{len(conversations)} ä»¶ã®ä¼šè©±ãŒæœ‰åŠ¹")
    
    # ãƒãƒƒãƒãƒˆãƒ¼ã‚¯ãƒ³åŒ–ï¼ˆåŠ¹ç‡çš„ï¼‰
    try:
        encodings = tokenizer(
            processed_texts,
            truncation=True,
            padding=True,
            max_length=max_length,
            return_tensors="pt"
        )
        
        print(f"ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å®Œäº†: {len(processed_texts)} ä»¶ã®ãƒ‡ãƒ¼ã‚¿")
        print(f"å¹³å‡ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {encodings['input_ids'].shape[1]}")
        
        return encodings
        
    except Exception as e:
        print(f"ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        raise

# å‰å‡¦ç†å®Ÿè¡Œ
max_length = 256  # Colabã®ãƒ¡ãƒ¢ãƒªåˆ¶é™ã‚’è€ƒæ…®
encodings = preprocess_conversations(conversations, tokenizer, max_length)

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
train_dataset = Dataset.from_dict({
    'input_ids': encodings['input_ids'],
    'attention_mask': encodings['attention_mask'],
    'labels': encodings['input_ids'].clone()  # è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã§ã¯labels=input_ids
})

print(f"å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆå®Œäº†: {len(train_dataset)} ä»¶")

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µãƒ³ãƒ—ãƒ«ã®ç¢ºèª
sample_idx = 0
sample_tokens = train_dataset[sample_idx]['input_ids']
sample_text = tokenizer.decode(sample_tokens, skip_special_tokens=False)
print(f"\nãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µãƒ³ãƒ—ãƒ«:\n{sample_text}")
```

## 5. å­¦ç¿’è¨­å®šã¨æœ€é©åŒ–

### è©³ç´°ãªå­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š

```python
def create_training_arguments(output_dir: str = './gpt2-chatbot') -> TrainingArguments:
    """
    Google Colabç’°å¢ƒã«æœ€é©åŒ–ã•ã‚ŒãŸå­¦ç¿’è¨­å®šã‚’ä½œæˆ
    """
    
    return TrainingArguments(
        # å‡ºåŠ›è¨­å®š
        output_dir=output_dir,
        overwrite_output_dir=True,
        
        # å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        num_train_epochs=5,  # ã‚¨ãƒãƒƒã‚¯æ•°ã‚’å¢—åŠ 
        per_device_train_batch_size=1,  # Colabãƒ¡ãƒ¢ãƒªåˆ¶é™å¯¾å¿œ
        gradient_accumulation_steps=8,  # å®ŸåŠ¹ãƒãƒƒãƒã‚µã‚¤ã‚º = 1 * 8 = 8
        
        # æœ€é©åŒ–è¨­å®š
        learning_rate=3e-5,  # GPT-2ã«é©ã—ãŸå­¦ç¿’ç‡
        weight_decay=0.01,   # éå­¦ç¿’é˜²æ­¢
        warmup_steps=200,    # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã‚¹ãƒ†ãƒƒãƒ—
        
        # ä¿å­˜ã¨ãƒ­ã‚°è¨­å®š
        save_strategy="steps",
        save_steps=100,
        save_total_limit=3,  # ä¿å­˜ãƒ¢ãƒ‡ãƒ«æ•°åˆ¶é™ï¼ˆã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ç¯€ç´„ï¼‰
        
        logging_dir=f'{output_dir}/logs',
        logging_steps=20,
        
        # è©•ä¾¡è¨­å®šï¼ˆä»Šå›ã¯è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ã¿ã®ãŸã‚ç„¡åŠ¹åŒ–ï¼‰
        evaluation_strategy="no",
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–
        dataloader_pin_memory=True,
        dataloader_num_workers=2,
        
        # Mixed Precision Training (GPUé«˜é€ŸåŒ–)
        fp16=torch.cuda.is_available(),
        
        # ãã®ä»–
        report_to=None,  # WandBãªã©ã®ãƒ­ã‚°ã‚µãƒ¼ãƒ“ã‚¹ç„¡åŠ¹åŒ–
        seed=42,
    )

# å­¦ç¿’è¨­å®šä½œæˆ
training_args = create_training_arguments()

# ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼è¨­å®š
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False,  # GPT-2ã¯Causal LMï¼ˆæ¬¡ã®å˜èªäºˆæ¸¬ï¼‰ãªã®ã§MLMã¯ç„¡åŠ¹
    return_tensors="pt"
)

print("å­¦ç¿’è¨­å®šå®Œäº†:")
print(f"  ã‚¨ãƒãƒƒã‚¯æ•°: {training_args.num_train_epochs}")
print(f"  ãƒãƒƒãƒã‚µã‚¤ã‚º: {training_args.per_device_train_batch_size}")
print(f"  å‹¾é…ç´¯ç©: {training_args.gradient_accumulation_steps}")
print(f"  å­¦ç¿’ç‡: {training_args.learning_rate}")
print(f"  FP16: {training_args.fp16}")
```

## 6. ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã®å®Ÿè¡Œ

### å …ç‰¢ãªå­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹

```python
def train_chatbot_model(model, tokenizer, train_dataset, training_args, data_collator):
    """
    ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚’å«ã‚€å …ç‰¢ãªå­¦ç¿’é–¢æ•°
    """
    
    # ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã®åˆæœŸåŒ–
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        data_collator=data_collator,
        tokenizer=tokenizer,
    )
    
    print("=== ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹ ===")
    print(f"å­¦ç¿’ãƒ‡ãƒ¼ã‚¿æ•°: {len(train_dataset)}")
    print(f"æ¨å®šå­¦ç¿’æ™‚é–“: {len(train_dataset) * training_args.num_train_epochs // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps) // 10} åˆ†ç¨‹åº¦")
    
    try:
        # å­¦ç¿’å®Ÿè¡Œ
        start_time = datetime.now()
        model.train()
        
        # å­¦ç¿’å±¥æ­´ã‚’å–å¾—
        train_result = trainer.train()
        
        end_time = datetime.now()
        training_duration = end_time - start_time
        
        print(f"\n=== å­¦ç¿’å®Œäº† ===")
        print(f"å­¦ç¿’æ™‚é–“: {training_duration}")
        print(f"æœ€çµ‚æå¤±: {train_result.training_loss:.4f}")
        
        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        print("ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ä¸­...")
        trainer.save_model()
        tokenizer.save_pretrained(training_args.output_dir)
        
        # å­¦ç¿’å±¥æ­´ä¿å­˜
        train_history = {
            'training_loss': train_result.training_loss,
            'training_duration': str(training_duration),
            'num_epochs': training_args.num_train_epochs,
            'learning_rate': training_args.learning_rate,
        }
        
        with open(f"{training_args.output_dir}/training_history.json", 'w') as f:
            json.dump(train_history, f, indent=2)
        
        print(f"ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ '{training_args.output_dir}' ã«ä¿å­˜ã—ã¾ã—ãŸ")
        return trainer, train_result
        
    except Exception as e:
        print(f"å­¦ç¿’ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
        print("ãƒ¡ãƒ¢ãƒªä¸è¶³ã®å ´åˆã¯ã€ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å°ã•ãã™ã‚‹ã‹ã€max_lengthã‚’çŸ­ãã—ã¦ãã ã•ã„")
        raise

# å­¦ç¿’å®Ÿè¡Œ
trainer, train_result = train_chatbot_model(
    model=model,
    tokenizer=tokenizer, 
    train_dataset=train_dataset,
    training_args=training_args,
    data_collator=data_collator
)
```

## 7. é«˜åº¦ãªãƒãƒ£ãƒƒãƒˆæ©Ÿèƒ½ã®å®Ÿè£…

### ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãªå¿œç­”ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ 

```python
class GPT2ChatBot:
    """
    é«˜æ©Ÿèƒ½ãªGPT-2ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã‚¯ãƒ©ã‚¹
    """
    
    def __init__(self, model, tokenizer, device, max_history=5):
        self.model = model
        self.tokenizer = tokenizer
        self.device = device
        self.conversation_history = []
        self.max_history = max_history
        
        # å¿œç­”ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.generation_params = {
            'max_new_tokens': 80,
            'temperature': 0.8,
            'top_p': 0.9,
            'top_k': 50,
            'repetition_penalty': 1.1,
            'do_sample': True,
            'pad_token_id': tokenizer.pad_token_id,
        }
        
    def generate_response(self, user_input: str, use_history: bool = True) -> str:
        """
        ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã«å¯¾ã—ã¦å¿œç­”ã‚’ç”Ÿæˆ
        
        Args:
            user_input: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®å…¥åŠ›
            use_history: ä¼šè©±å±¥æ­´ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹
            
        Returns:
            ç”Ÿæˆã•ã‚ŒãŸå¿œç­”ãƒ†ã‚­ã‚¹ãƒˆ
        """
        try:
            # å…¥åŠ›ã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
            user_input = user_input.strip()
            if not user_input:
                return "ä½•ã‹ãŠè©±ã—ãã ã•ã„ã€‚"
            
            # ä¼šè©±æ–‡è„ˆã®æ§‹ç¯‰
            context = self._build_context(user_input, use_history)
            
            # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
            inputs = self.tokenizer.encode(context, return_tensors='pt').to(self.device)
            
            # å¿œç­”ç”Ÿæˆ
            self.model.eval()
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs,
                    **self.generation_params
                )
            
            # å¿œç­”ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=False)
            response = self._extract_bot_response(generated_text)
            
            # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            response = self._filter_response(response)
            
            # å±¥æ­´æ›´æ–°
            self._update_history(user_input, response)
            
            return response
            
        except Exception as e:
            print(f"å¿œç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚å¿œç­”ã®ç”Ÿæˆã«å•é¡ŒãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"
    
    def _build_context(self, user_input: str, use_history: bool) -> str:
        """ä¼šè©±æ–‡è„ˆã‚’æ§‹ç¯‰"""
        context = ""
        
        if use_history and self.conversation_history:
            # ç›´è¿‘ã®ä¼šè©±å±¥æ­´ã‚’ä½¿ç”¨
            recent_history = self.conversation_history[-self.max_history:]
            for turn in recent_history:
                context += f"<|user|>{turn['user']}<|bot|>{turn['bot']}"
        
        context += f"<|user|>{user_input}<|bot|>"
        return context
    
    def _extract_bot_response(self, generated_text: str) -> str:
        """ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãƒœãƒƒãƒˆã®å¿œç­”ã‚’æŠ½å‡º"""
        try:
            # æœ€å¾Œã®<|bot|>ä»¥é™ã‚’å–å¾—
            if '<|bot|>' in generated_text:
                response = generated_text.split('<|bot|>')[-1]
                # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã§çµ‚äº†
                response = response.split('<|endoftext|>')[0]
                response = response.split('<|user|>')[0]
                return response.strip()
            else:
                return "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚é©åˆ‡ãªå¿œç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
        except:
            return "å¿œç­”ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"
    
    def _filter_response(self, response: str) -> str:
        """å¿œç­”ã®å“è³ªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
        # ç©ºã¾ãŸã¯çŸ­ã™ãã‚‹å¿œç­”
        if len(response.strip()) < 2:
            return "ã‚‚ã†å°‘ã—è©³ã—ãæ•™ãˆã¦ãã ã•ã„ã€‚"
        
        # é•·ã™ãã‚‹å¿œç­”
        if len(response) > 200:
            sentences = response.split('ã€‚')
            if len(sentences) > 1:
                response = 'ã€‚'.join(sentences[:2]) + 'ã€‚'
        
        # é‡è¤‡å˜èªã®ãƒã‚§ãƒƒã‚¯
        words = response.split()
        if len(words) > 5:
            unique_ratio = len(set(words)) / len(words)
            if unique_ratio < 0.3:
                return "åˆ¥ã®æ–¹æ³•ã§èª¬æ˜ã„ãŸã—ã¾ã™ã€‚"
        
        return response
    
    def _update_history(self, user_input: str, bot_response: str):
        """ä¼šè©±å±¥æ­´ã‚’æ›´æ–°"""
        self.conversation_history.append({
            'user': user_input,
            'bot': bot_response
        })
        
        # å±¥æ­´é•·ã®åˆ¶é™
        if len(self.conversation_history) > self.max_history:
            self.conversation_history.pop(0)
    
    def interactive_chat(self):
        """ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒãƒ£ãƒƒãƒˆæ©Ÿèƒ½"""
        print("=" * 50)
        print("GPT-2 ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆï¼ˆæ”¹è‰¯ç‰ˆï¼‰")
        print("=" * 50)
        print("çµ‚äº†ã‚³ãƒãƒ³ãƒ‰: quit, exit, bye, q")
        print("å±¥æ­´ã‚¯ãƒªã‚¢: clear")
        print("è¨­å®šå¤‰æ›´: config")
        print("-" * 50)
        
        while True:
            try:
                user_input = input("\nğŸ§‘ ã‚ãªãŸ: ").strip()
                
                if not user_input:
                    continue
                
                # çµ‚äº†ã‚³ãƒãƒ³ãƒ‰
                if user_input.lower() in ['quit', 'exit', 'bye', 'q']:
                    print("\nğŸ¤– ãƒœãƒƒãƒˆ: ã•ã‚ˆã†ãªã‚‰ï¼ã¾ãŸè©±ã—ã¾ã—ã‚‡ã†ã€‚")
                    break
                
                # å±¥æ­´ã‚¯ãƒªã‚¢ã‚³ãƒãƒ³ãƒ‰
                if user_input.lower() == 'clear':
                    self.conversation_history = []
                    print("\nğŸ¤– ãƒœãƒƒãƒˆ: ä¼šè©±å±¥æ­´ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸã€‚")
                    continue
                
                # è¨­å®šå¤‰æ›´ã‚³ãƒãƒ³ãƒ‰
                if user_input.lower() == 'config':
                    self._config_menu()
                    continue
                
                # å¿œç­”ç”Ÿæˆ
                print("\nğŸ¤– ãƒœãƒƒãƒˆ: ", end="", flush=True)
                response = self.generate_response(user_input)
                print(response)
                
            except KeyboardInterrupt:
                print("\n\nğŸ¤– ãƒœãƒƒãƒˆ: ãƒãƒ£ãƒƒãƒˆã‚’çµ‚äº†ã—ã¾ã™ã€‚ãŠç–²ã‚Œã•ã¾ã§ã—ãŸï¼")
                break
            except Exception as e:
                print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                continue
        
        return self.conversation_history
    
    def _config_menu(self):
        """è¨­å®šå¤‰æ›´ãƒ¡ãƒ‹ãƒ¥ãƒ¼"""
        print("\n=== è¨­å®šå¤‰æ›´ ===")
        print(f"1. æ¸©åº¦è¨­å®š (ç¾åœ¨: {self.generation_params['temperature']})")
        print(f"2. å±¥æ­´é•· (ç¾åœ¨: {self.max_history})")
        print(f"3. æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•° (ç¾åœ¨: {self.generation_params['max_new_tokens']})")
        print("0. æˆ»ã‚‹")
        
        try:
            choice = input("é¸æŠã—ã¦ãã ã•ã„ (0-3): ").strip()
            
            if choice == '1':
                temp = float(input("æ¸©åº¦ (0.1-2.0): "))
                if 0.1 <= temp <= 2.0:
                    self.generation_params['temperature'] = temp
                    print(f"æ¸©åº¦ã‚’ {temp} ã«è¨­å®šã—ã¾ã—ãŸã€‚")
                else:
                    print("ç„¡åŠ¹ãªå€¤ã§ã™ã€‚")
                    
            elif choice == '2':
                history_len = int(input("å±¥æ­´é•· (1-10): "))
                if 1 <= history_len <= 10:
                    self.max_history = history_len
                    print(f"å±¥æ­´é•·ã‚’ {history_len} ã«è¨­å®šã—ã¾ã—ãŸã€‚")
                else:
                    print("ç„¡åŠ¹ãªå€¤ã§ã™ã€‚")
                    
            elif choice == '3':
                max_tokens = int(input("æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•° (20-150): "))
                if 20 <= max_tokens <= 150:
                    self.generation_params['max_new_tokens'] = max_tokens
                    print(f"æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ {max_tokens} ã«è¨­å®šã—ã¾ã—ãŸã€‚")
                else:
                    print("ç„¡åŠ¹ãªå€¤ã§ã™ã€‚")
                    
        except ValueError:
            print("ç„¡åŠ¹ãªå…¥åŠ›ã§ã™ã€‚")

# ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆ
chatbot = GPT2ChatBot(model, tokenizer, device)

print("ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆæº–å‚™å®Œäº†ï¼")
```

## 8. ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ã¨æ€§èƒ½æ¸¬å®š

### åŒ…æ‹¬çš„è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ 

```python
class ChatBotEvaluator:
    """
    ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã®æ€§èƒ½ã‚’å¤šè§’çš„ã«è©•ä¾¡ã™ã‚‹ã‚¯ãƒ©ã‚¹
    """
    
    def __init__(self, chatbot: GPT2ChatBot):
        self.chatbot = chatbot
        self.evaluation_results = {}
    
    def evaluate_comprehensive(self) -> Dict:
        """åŒ…æ‹¬çš„ãªè©•ä¾¡ã‚’å®Ÿè¡Œ"""
        
        print("=== ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆç·åˆè©•ä¾¡é–‹å§‹ ===\n")
        
        # 1. å¿œç­”å“è³ªè©•ä¾¡
        self._evaluate_response_quality()
        
        # 2. ä¸€è²«æ€§è©•ä¾¡
        self._evaluate_consistency()
        
        # 3. å¤šæ§˜æ€§è©•ä¾¡
        self._evaluate_diversity()
        
        # 4. å¿œç­”é€Ÿåº¦è©•ä¾¡
        self._evaluate_response_time()
        
        # 5. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°è©•ä¾¡
        self._evaluate_error_handling()
        
        # çµæœã‚µãƒãƒªãƒ¼
        self._generate_evaluation_summary()
        
        return self.evaluation_results
    
    def _evaluate_response_quality(self):
        """å¿œç­”å“è³ªã®è©•ä¾¡"""
        print("1. å¿œç­”å“è³ªè©•ä¾¡")
        
        test_cases = [
            {"input": "ãŠã¯ã‚ˆã†", "expected_keywords": ["ãŠã¯ã‚ˆã†", "ã“ã‚“ã«ã¡ã¯", "ä»Šæ—¥"]},
            {"input": "ã‚ã‚ŠãŒã¨ã†", "expected_keywords": ["ã©ã†ã„ãŸã—ã¾ã—ã¦", "ãŠå½¹", "å¬‰ã—ã„"]},
            {"input": "ç–²ã‚Œã¾ã—ãŸ", "expected_keywords": ["ãŠç–²ã‚Œ", "ä¼‘æ†©", "å¤§å¤‰"]},
            {"input": "æ˜ ç”»ã‚’è¦‹ãŸã„", "expected_keywords": ["æ˜ ç”»", "ã‚¸ãƒ£ãƒ³ãƒ«", "ãŠã™ã™ã‚"]},
            {"input": "å‹‰å¼·ã«ã¤ã„ã¦", "expected_keywords": ["å‹‰å¼·", "å­¦ç¿’", "åŠ¹ç‡"]},
        ]
        
        quality_scores = []
        
        for i, case in enumerate(test_cases, 1):
            response = self.chatbot.generate_response(case["input"], use_history=False)
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é–¢é€£æ€§ãƒã‚§ãƒƒã‚¯
            keyword_score = self._calculate_keyword_relevance(
                response, case["expected_keywords"]
            )
            
            # å¿œç­”é•·ãƒã‚§ãƒƒã‚¯
            length_score = self._calculate_length_score(response)
            
            # è‡ªç„¶ã•ãƒã‚§ãƒƒã‚¯
            naturalness_score = self._calculate_naturalness_score(response)
            
            total_score = (keyword_score + length_score + naturalness_score) / 3
            quality_scores.append(total_score)
            
            print(f"  ãƒ†ã‚¹ãƒˆ {i}: ã€Œ{case['input']}ã€")
            print(f"    å¿œç­”: ã€Œ{response}ã€")
            print(f"    å“è³ªã‚¹ã‚³ã‚¢: {total_score:.2f}/1.0")
            print()
        
        avg_quality = sum(quality_scores) / len(quality_scores)
        self.evaluation_results['response_quality'] = {
            'average_score': avg_quality,
            'individual_scores': quality_scores,
            'grade': self._score_to_grade(avg_quality)
        }
        
        print(f"å¿œç­”å“è³ªå¹³å‡: {avg_quality:.2f}/1.0 ({self._score_to_grade(avg_quality)})")
        print("-" * 50)
    
    def _evaluate_consistency(self):
        """ä¸€è²«æ€§è©•ä¾¡"""
        print("2. ä¸€è²«æ€§è©•ä¾¡")
        
        # åŒã˜è³ªå•ã‚’è¤‡æ•°å›å®Ÿè¡Œ
        test_input = "ã“ã‚“ã«ã¡ã¯"
        responses = []
        
        for i in range(5):
            response = self.chatbot.generate_response(test_input, use_history=False)
            responses.append(response)
            print(f"  è©¦è¡Œ {i+1}: ã€Œ{response}ã€")
        
        # å¿œç­”ã®å¤šæ§˜æ€§ã¨ä¸€è²«æ€§ã®ãƒãƒ©ãƒ³ã‚¹ã‚’è©•ä¾¡
        consistency_score = self._calculate_consistency_score(responses)
        
        self.evaluation_results['consistency'] = {
            'score': consistency_score,
            'responses': responses,
            'grade': self._score_to_grade(consistency_score)
        }
        
        print(f"ä¸€è²«æ€§ã‚¹ã‚³ã‚¢: {consistency_score:.2f}/1.0 ({self._score_to_grade(consistency_score)})")
        print("-" * 50)
    
    def _evaluate_diversity(self):
        """å¤šæ§˜æ€§è©•ä¾¡"""
        print("3. å¿œç­”å¤šæ§˜æ€§è©•ä¾¡")
        
        diverse_inputs = [
            "å¤©æ°—ã«ã¤ã„ã¦", "éŸ³æ¥½ã«ã¤ã„ã¦", "æ–™ç†ã«ã¤ã„ã¦", "æ—…è¡Œã«ã¤ã„ã¦", "èª­æ›¸ã«ã¤ã„ã¦"
        ]
        
        responses = []
        for inp in diverse_inputs:
            response = self.chatbot.generate_response(inp, use_history=False)
            responses.append(response)
            print(f"  ã€Œ{inp}ã€â†’ã€Œ{response[:50]}...ã€")
        
        diversity_score = self._calculate_diversity_score(responses)
        
        self.evaluation_results['diversity'] = {
            'score': diversity_score,
            'grade': self._score_to_grade(diversity_score)
        }
        
        print(f"å¤šæ§˜æ€§ã‚¹ã‚³ã‚¢: {diversity_score:.2f}/1.0 ({self._score_to_grade(diversity_score)})")
        print("-" * 50)
    
    def _evaluate_response_time(self):
        """å¿œç­”é€Ÿåº¦è©•ä¾¡"""
        print("4. å¿œç­”é€Ÿåº¦è©•ä¾¡")
        
        import time
        
        test_inputs = ["ã“ã‚“ã«ã¡ã¯", "å…ƒæ°—ã§ã™ã‹", "ä»Šæ—¥ã¯ä½•ã‚’ã—ã¦ã„ã¾ã—ãŸã‹"]
        response_times = []
        
        for inp in test_inputs:
            start_time = time.time()
            response = self.chatbot.generate_response(inp, use_history=False)
            end_time = time.time()
            
            response_time = end_time - start_time
            response_times.append(response_time)
            
            print(f"  ã€Œ{inp}ã€: {response_time:.2f}ç§’")
        
        avg_time = sum(response_times) / len(response_times)
        speed_score = max(0, min(1, (3.0 - avg_time) / 3.0))  # 3ç§’ä»¥ä¸‹ãŒæº€ç‚¹
        
        self.evaluation_results['response_time'] = {
            'average_time': avg_time,
            'individual_times': response_times,
            'score': speed_score,
            'grade': self._score_to_grade(speed_score)
        }
        
        print(f"å¹³å‡å¿œç­”æ™‚é–“: {avg_time:.2f}ç§’")
        print(f"é€Ÿåº¦ã‚¹ã‚³ã‚¢: {speed_score:.2f}/1.0 ({self._score_to_grade(speed_score)})")
        print("-" * 50)
    
    def _evaluate_error_handling(self):
        """ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°è©•ä¾¡"""
        print("5. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°è©•ä¾¡")
        
        error_cases = [
            "",  # ç©ºå…¥åŠ›
            "a" * 500,  # æ¥µç«¯ã«é•·ã„å…¥åŠ›
            "ğŸ‰ğŸŠâœ¨ğŸŒŸğŸ’«",  # çµµæ–‡å­—ã®ã¿
            "1234567890",  # æ•°å­—ã®ã¿
            "!@#$%^&*()",  # è¨˜å·ã®ã¿
        ]
        
        error_handling_scores = []
        
        for i, case in enumerate(error_cases, 1):
            try:
                response = self.chatbot.generate_response(case, use_history=False)
                
                # å¿œç­”ãŒé©åˆ‡ã«ç”Ÿæˆã•ã‚ŒãŸã‹ãƒã‚§ãƒƒã‚¯
                if response and "ã‚¨ãƒ©ãƒ¼" not in response and "ç”³ã—è¨³" in response:
                    score = 1.0
                elif response:
                    score = 0.7
                else:
                    score = 0.3
                    
                error_handling_scores.append(score)
                
                case_display = case[:20] + "..." if len(case) > 20 else case
                print(f"  ã‚±ãƒ¼ã‚¹ {i}: ã€Œ{case_display}ã€â†’ ã‚¹ã‚³ã‚¢: {score:.1f}")
                
            except Exception as e:
                error_handling_scores.append(0.0)
                print(f"  ã‚±ãƒ¼ã‚¹ {i}: ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ - {e}")
        
        avg_error_handling = sum(error_handling_scores) / len(error_handling_scores)
        
        self.evaluation_results['error_handling'] = {
            'average_score': avg_error_handling,
            'individual_scores': error_handling_scores,
            'grade': self._score_to_grade(avg_error_handling)
        }
        
        print(f"ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚¹ã‚³ã‚¢: {avg_error_handling:.2f}/1.0 ({self._score_to_grade(avg_error_handling)})")
        print("-" * 50)
    
    def _calculate_keyword_relevance(self, response: str, keywords: List[str]) -> float:
        """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é–¢é€£æ€§ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        response_lower = response.lower()
        matches = sum(1 for keyword in keywords if keyword in response_lower)
        return min(1.0, matches / len(keywords))
    
    def _calculate_length_score(self, response: str) -> float:
        """é©åˆ‡ãªå¿œç­”é•·ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        length = len(response)
        if 10 <= length <= 100:
            return 1.0
        elif 5 <= length <= 150:
            return 0.7
        else:
            return 0.3
    
    def _calculate_naturalness_score(self, response: str) -> float:
        """è‡ªç„¶ã•ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        # åŸºæœ¬çš„ãªãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯
        if not response or len(response) < 2:
            return 0.0
        
        # æ—¥æœ¬èªã®åŸºæœ¬çš„ãªæ–‡ç« æ§‹é€ ãƒã‚§ãƒƒã‚¯
        has_hiragana = any('ã²' <= c <= 'ã‚Ÿ' for c in response)
        has_punctuation = any(c in response for c in 'ã€‚ï¼ï¼Ÿ')
        
        score = 0.5
        if has_hiragana:
            score += 0.3
        if has_punctuation:
            score += 0.2
        
        return min(1.0, score)
    
    def _calculate_consistency_score(self, responses: List[str]) -> float:
        """ä¸€è²«æ€§ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        if len(responses) < 2:
            return 0.0
        
        # å¿œç­”ã®é¡ä¼¼åº¦ã‚’ç°¡æ˜“è¨ˆç®—
        total_similarity = 0
        comparisons = 0
        
        for i in range(len(responses)):
            for j in range(i + 1, len(responses)):
                similarity = self._simple_similarity(responses[i], responses[j])
                total_similarity += similarity
                comparisons += 1
        
        avg_similarity = total_similarity / comparisons if comparisons > 0 else 0
        
        # é©åº¦ãªä¸€è²«æ€§ï¼ˆ0.3-0.7ï¼‰ãŒç†æƒ³
        if 0.3 <= avg_similarity <= 0.7:
            return 1.0
        else:
            return max(0.0, 1.0 - abs(avg_similarity - 0.5) * 2)
    
    def _calculate_diversity_score(self, responses: List[str]) -> float:
        """å¤šæ§˜æ€§ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        if len(responses) < 2:
            return 0.0
        
        # å…¨å¿œç­”é–“ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—
        total_similarity = 0
        comparisons = 0
        
        for i in range(len(responses)):
            for j in range(i + 1, len(responses)):
                similarity = self._simple_similarity(responses[i], responses[j])
                total_similarity += similarity
                comparisons += 1
        
        avg_similarity = total_similarity / comparisons if comparisons > 0 else 0
        
        # ä½ã„é¡ä¼¼åº¦ï¼ˆé«˜ã„å¤šæ§˜æ€§ï¼‰ãŒè‰¯ã„
        return max(0.0, 1.0 - avg_similarity)
    
    def _simple_similarity(self, text1: str, text2: str) -> float:
        """ç°¡æ˜“æ–‡å­—åˆ—é¡ä¼¼åº¦è¨ˆç®—"""
        words1 = set(text1.split())
        words2 = set(text2.split())
        
        if not words1 and not words2:
            return 1.0
        if not words1 or not words2:
            return 0.0
        
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        return len(intersection) / len(union)
    
    def _score_to_grade(self, score: float) -> str:
        """ã‚¹ã‚³ã‚¢ã‚’ã‚°ãƒ¬ãƒ¼ãƒ‰ã«å¤‰æ›"""
        if score >= 0.9:
            return "å„ªç§€"
        elif score >= 0.7:
            return "è‰¯å¥½"
        elif score >= 0.5:
            return "æ™®é€š"
        elif score >= 0.3:
            return "æ”¹å–„è¦"
        else:
            return "è¦æ”¹è‰¯"
    
    def _generate_evaluation_summary(self):
        """è©•ä¾¡çµæœã®ã‚µãƒãƒªãƒ¼ç”Ÿæˆ"""
        print("=== ç·åˆè©•ä¾¡çµæœ ===")
        
        categories = [
            ('å¿œç­”å“è³ª', 'response_quality'),
            ('ä¸€è²«æ€§', 'consistency'),
            ('å¤šæ§˜æ€§', 'diversity'),
            ('å¿œç­”é€Ÿåº¦', 'response_time'),
            ('ã‚¨ãƒ©ãƒ¼å‡¦ç†', 'error_handling')
        ]
        
        total_score = 0
        for name, key in categories:
            score = self.evaluation_results[key]['score'] if key == 'response_time' else self.evaluation_results[key].get('average_score', self.evaluation_results[key]['score'])
            grade = self.evaluation_results[key]['grade']
            print(f"{name}: {score:.2f}/1.0 ({grade})")
            total_score += score
        
        overall_score = total_score / len(categories)
        overall_grade = self._score_to_grade(overall_score)
        
        print(f"\nç·åˆã‚¹ã‚³ã‚¢: {overall_score:.2f}/1.0 ({overall_grade})")
        
        # æ”¹å–„ææ¡ˆ
        print(f"\n=== æ”¹å–„ææ¡ˆ ===")
        self._generate_improvement_suggestions()
        
        self.evaluation_results['overall'] = {
            'score': overall_score,
            'grade': overall_grade
        }
    
    def _generate_improvement_suggestions(self):
        """æ”¹å–„ææ¡ˆã®ç”Ÿæˆ"""
        suggestions = []
        
        if self.evaluation_results['response_quality']['average_score'] < 0.7:
            suggestions.append("â€¢ ã‚ˆã‚Šå¤šæ§˜ã§é«˜å“è³ªãªå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ ")
            suggestions.append("â€¢ å­¦ç¿’ã‚¨ãƒãƒƒã‚¯æ•°ã®èª¿æ•´")
        
        if self.evaluation_results['consistency']['score'] < 0.6:
            suggestions.append("â€¢ æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®èª¿æ•´ï¼ˆã‚ˆã‚Šå®‰å®šã—ãŸå‡ºåŠ›ã®ãŸã‚ï¼‰")
            suggestions.append("â€¢ å¿œç­”ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã®å¼·åŒ–")
        
        if self.evaluation_results['diversity']['score'] < 0.5:
            suggestions.append("â€¢ top_pã‚„top_kãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®èª¿æ•´")
            suggestions.append("â€¢ ã‚ˆã‚Šå¤šæ§˜ãªå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®ä½¿ç”¨")
        
        if self.evaluation_results['response_time']['score'] < 0.7:
            suggestions.append("â€¢ max_new_tokensã®å‰Šæ¸›")
            suggestions.append("â€¢ GPUã®ä½¿ç”¨ç¢ºèª")
        
        if suggestions:
            for suggestion in suggestions:
                print(suggestion)
        else:
            print("ç¾åœ¨ã®æ€§èƒ½ã¯è‰¯å¥½ã§ã™ã€‚ç¶™ç¶šçš„ãªç›£è¦–ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚")

# è©•ä¾¡å®Ÿè¡Œ
evaluator = ChatBotEvaluator(chatbot)
evaluation_results = evaluator.evaluate_comprehensive()
```

## 9. å®Ÿç”¨çš„ãªãƒ‡ãƒ¢ã¨ãƒ†ã‚¹ãƒˆ

### ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ‡ãƒ¢ã‚·ã‚¹ãƒ†ãƒ 

```python
def run_comprehensive_demo():
    """
    åŒ…æ‹¬çš„ãªãƒ‡ãƒ¢ã¨ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
    """
    
    print("=" * 60)
    print("GPT-2ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ åŒ…æ‹¬çš„ãƒ‡ãƒ¢")
    print("=" * 60)
    
    # 1. åŸºæœ¬æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ
    print("\n1. åŸºæœ¬å¿œç­”ãƒ†ã‚¹ãƒˆ")
    print("-" * 30)
    
    basic_tests = [
        "ã“ã‚“ã«ã¡ã¯",
        "ä»Šæ—¥ã®èª¿å­ã¯ã©ã†ã§ã™ã‹ï¼Ÿ",
        "ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™",
        "æ˜ ç”»ã«ã¤ã„ã¦æ•™ãˆã¦",
        "ã•ã‚ˆã†ãªã‚‰"
    ]
    
    for test_input in basic_tests:
        response = chatbot.generate_response(test_input, use_history=False)
        print(f"å…¥åŠ›: {test_input}")
        print(f"å¿œç­”: {response}")
        print()
    
    # 2. ä¼šè©±å±¥æ­´æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ
    print("\n2. ä¼šè©±å±¥æ­´æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ")
    print("-" * 30)
    
    # å±¥æ­´ã‚’ã‚¯ãƒªã‚¢
    chatbot.conversation_history = []
    
    conversation_flow = [
        "ç§ã®åå‰ã¯ç”°ä¸­ã§ã™",
        "ä»Šæ—¥ã¯è‰¯ã„å¤©æ°—ã§ã™ã­",
        "ç§ã®åå‰ã‚’è¦šãˆã¦ã„ã¾ã™ã‹ï¼Ÿ"
    ]
    
    for turn in conversation_flow:
        response = chatbot.generate_response(turn, use_history=True)
        print(f"å…¥åŠ›: {turn}")
        print(f"å¿œç­”: {response}")
        print(f"å±¥æ­´æ•°: {len(chatbot.conversation_history)}")
        print()
    
    # 3. è¨­å®šå¤‰æ›´ãƒ†ã‚¹ãƒˆ
    print("\n3. ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¯”è¼ƒãƒ†ã‚¹ãƒˆ")
    print("-" * 30)
    
    test_input = "ãŠã™ã™ã‚ã®æœ¬ã‚’æ•™ãˆã¦"
    
    # å…ƒã®è¨­å®šã‚’ä¿å­˜
    original_temp = chatbot.generation_params['temperature']
    
    temperatures = [0.3, 0.7, 1.0]
    
    for temp in temperatures:
        chatbot.generation_params['temperature'] = temp
        response = chatbot.generate_response(test_input, use_history=False)
        print(f"æ¸©åº¦ {temp}: {response}")
        print()
    
    # è¨­å®šã‚’å¾©å…ƒ
    chatbot.generation_params['temperature'] = original_temp
    
    # 4. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆ
    print("\n4. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆ")
    print("-" * 30)
    
    error_tests = ["", "???", "1234567890"]
    
    for error_input in error_tests:
        try:
            response = chatbot.generate_response(error_input, use_history=False)
            print(f"å…¥åŠ›: '{error_input}' â†’ å¿œç­”: {response}")
        except Exception as e:
            print(f"å…¥åŠ›: '{error_input}' â†’ ã‚¨ãƒ©ãƒ¼: {e}")
    
    print("\n" + "=" * 60)
    print("ãƒ‡ãƒ¢å®Œäº†ï¼")
    print("=" * 60)

# ãƒ‡ãƒ¢å®Ÿè¡Œ
run_comprehensive_demo()
```

## 10. é‹ç”¨ã¨ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹

### ç¶™ç¶šçš„æ”¹å–„ã‚·ã‚¹ãƒ†ãƒ 

```python
class ChatBotMaintenanceSystem:
    """
    ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã®é‹ç”¨ãƒ»ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ã‚’æ”¯æ´ã™ã‚‹ã‚¯ãƒ©ã‚¹
    """
    
    def __init__(self, chatbot: GPT2ChatBot):
        self.chatbot = chatbot
        self.usage_stats = {
            'total_conversations': 0,
            'total_responses': 0,
            'average_response_length': 0,
            'common_inputs': {},
            'error_count': 0,
            'session_start': datetime.now()
        }
        self.feedback_log = []
    
    def log_interaction(self, user_input: str, bot_response: str, error: bool = False):
        """ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³æƒ…å ±ã‚’ãƒ­ã‚°"""
        self.usage_stats['total_responses'] += 1
        
        if error:
            self.usage_stats['error_count'] += 1
        
        # å…¥åŠ›ãƒ‘ã‚¿ãƒ¼ãƒ³ã®çµ±è¨ˆ
        if user_input in self.usage_stats['common_inputs']:
            self.usage_stats['common_inputs'][user_input] += 1
        else:
            self.usage_stats['common_inputs'][user_input] = 1
        
        # å¹³å‡å¿œç­”é•·ã®æ›´æ–°
        current_avg = self.usage_stats['average_response_length']
        total = self.usage_stats['total_responses']
        new_avg = ((current_avg * (total - 1)) + len(bot_response)) / total
        self.usage_stats['average_response_length'] = new_avg
    
    def collect_feedback(self, user_input: str, bot_response: str, rating: int, comment: str = ""):
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’åé›†"""
        feedback = {
            'timestamp': datetime.now(),
            'user_input': user_input,
            'bot_response': bot_response,
            'rating': rating,  # 1-5
            'comment': comment
        }
        self.feedback_log.append(feedback)
    
    def generate_usage_report(self) -> str:
        """ä½¿ç”¨çŠ¶æ³ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        duration = datetime.now() - self.usage_stats['session_start']
        
        report = f"""
=== ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆä½¿ç”¨çŠ¶æ³ãƒ¬ãƒãƒ¼ãƒˆ ===

ã‚»ãƒƒã‚·ãƒ§ãƒ³æœŸé–“: {duration}
ç·å¿œç­”æ•°: {self.usage_stats['total_responses']}
ã‚¨ãƒ©ãƒ¼æ•°: {self.usage_stats['error_count']}
ã‚¨ãƒ©ãƒ¼ç‡: {(self.usage_stats['error_count'] / max(1, self.usage_stats['total_responses'])) * 100:.1f}%
å¹³å‡å¿œç­”é•·: {self.usage_stats['average_response_length']:.1f}æ–‡å­—

=== ã‚ˆãä½¿ã‚ã‚Œã‚‹å…¥åŠ› TOP5 ===
"""
        
        sorted_inputs = sorted(
            self.usage_stats['common_inputs'].items(),
            key=lambda x: x[1],
            reverse=True
        )[:5]
        
        for i, (input_text, count) in enumerate(sorted_inputs, 1):
            report += f"{i}. '{input_text}' ({count}å›)\n"
        
        if self.feedback_log:
            avg_rating = sum(f['rating'] for f in self.feedback_log) / len(self.feedback_log)
            report += f"\n=== ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯çµ±è¨ˆ ===\n"
            report += f"ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯æ•°: {len(self.feedback_log)}\n"
            report += f"å¹³å‡è©•ä¾¡: {avg_rating:.1f}/5.0\n"
        
        return report
    
    def suggest_improvements(self) -> List[str]:
        """æ”¹å–„ææ¡ˆã‚’ç”Ÿæˆ"""
        suggestions = []
        
        error_rate = (self.usage_stats['error_count'] / max(1, self.usage_stats['total_responses'])) * 100
        
        if error_rate > 10:
            suggestions.append("ã‚¨ãƒ©ãƒ¼ç‡ãŒé«˜ã„ã§ã™ã€‚å…¥åŠ›æ¤œè¨¼ã®å¼·åŒ–ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚")
        
        if self.usage_stats['average_response_length'] > 150:
            suggestions.append("å¿œç­”ãŒé•·ã™ãã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚max_new_tokensã®èª¿æ•´ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚")
        elif self.usage_stats['average_response_length'] < 20:
            suggestions.append("å¿œç­”ãŒçŸ­ã™ãã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®èª¿æ•´ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚")
        
        if self.feedback_log:
            avg_rating = sum(f['rating'] for f in self.feedback_log) / len(self.feedback_log)
            if avg_rating < 3.0:
                suggestions.append("ãƒ¦ãƒ¼ã‚¶ãƒ¼æº€è¶³åº¦ãŒä½ã„ã§ã™ã€‚å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®è¦‹ç›´ã—ã‚’æ¨å¥¨ã—ã¾ã™ã€‚")
        
        # ã‚ˆãä½¿ã‚ã‚Œã‚‹å…¥åŠ›ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ
        if self.usage_stats['common_inputs']:
            most_common = max(self.usage_stats['common_inputs'].items(), key=lambda x: x[1])
            total_responses = self.usage_stats['total_responses']
            if most_common[1] / total_responses > 0.3:
                suggestions.append(f"'{most_common[0]}'ã®ã‚ˆã†ãªå…¥åŠ›ãŒå¤šã„ã§ã™ã€‚ã“ã®åˆ†é‡ã®å¿œç­”å“è³ªå‘ä¸Šã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚")
        
        return suggestions if suggestions else ["ç¾åœ¨ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¯è‰¯å¥½ã§ã™ã€‚"]
    
    def export_data(self, filepath: str):
        """çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        export_data = {
            'usage_stats': self.usage_stats,
            'feedback_log': [
                {
                    **feedback,
                    'timestamp': feedback['timestamp'].isoformat()
                }
                for feedback in self.feedback_log
            ],
            'export_timestamp': datetime.now().isoformat()
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, ensure_ascii=False, indent=2)
        
        print(f"ãƒ‡ãƒ¼ã‚¿ã‚’ {filepath} ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ã¾ã—ãŸã€‚")

# ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–
maintenance = ChatBotMaintenanceSystem(chatbot)

print("ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ã‚·ã‚¹ãƒ†ãƒ ãŒåˆæœŸåŒ–ã•ã‚Œã¾ã—ãŸã€‚")
```

## 11. æœ€çµ‚çš„ãªä½¿ç”¨æ–¹æ³•ã¨ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

### å®Œå…¨ãªå®Ÿè¡Œä¾‹

```python
def main_execution_example():
    """
    å®Œå…¨ãªå®Ÿè¡Œä¾‹ã¨ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã®å®Ÿæ¼”
    """
    
    print("=" * 70)
    print("GPT-2ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ å®Œå…¨å®Ÿè¡Œã‚¬ã‚¤ãƒ‰")
    print("=" * 70)
    
    # ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ¢ãƒ‡ãƒ«ã®çŠ¶æ…‹ç¢ºèª
    print("\nğŸ“‹ ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ¢ãƒ‡ãƒ«çŠ¶æ…‹ç¢ºèª")
    print(f"ãƒ¢ãƒ‡ãƒ«: {model.__class__.__name__}")
    print(f"ãƒ‡ãƒã‚¤ã‚¹: {next(model.parameters()).device}")
    print(f"å­¦ç¿’ãƒ¢ãƒ¼ãƒ‰: {model.training}")
    
    # ã‚¹ãƒ†ãƒƒãƒ—2: åŸºæœ¬å‹•ä½œç¢ºèª
    print("\nğŸ§ª ã‚¹ãƒ†ãƒƒãƒ—2: åŸºæœ¬å‹•ä½œç¢ºèª")
    test_response = chatbot.generate_response("å‹•ä½œãƒ†ã‚¹ãƒˆã§ã™", use_history=False)
    print(f"ãƒ†ã‚¹ãƒˆå¿œç­”: {test_response}")
    
    # ã‚¹ãƒ†ãƒƒãƒ—3: è¨­å®šç¢ºèªã¨æœ€é©åŒ–
    print("\nâš™ï¸ ã‚¹ãƒ†ãƒƒãƒ—3: è¨­å®šæœ€é©åŒ–")
    print("ç¾åœ¨ã®ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:")
    for key, value in chatbot.generation_params.items():
        print(f"  {key}: {value}")
    
    # Colabç’°å¢ƒã«æœ€é©åŒ–ã•ã‚ŒãŸè¨­å®šã‚’é©ç”¨
    optimized_params = {
        'max_new_tokens': 60,  # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã®ãŸã‚å‰Šæ¸›
        'temperature': 0.7,    # ãƒãƒ©ãƒ³ã‚¹é‡è¦–
        'top_p': 0.9,
        'top_k': 50,
        'repetition_penalty': 1.1,
        'do_sample': True,
    }
    
    chatbot.generation_params.update(optimized_params)
    print("æœ€é©åŒ–ã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’é©ç”¨ã—ã¾ã—ãŸã€‚")
    
    # ã‚¹ãƒ†ãƒƒãƒ—4: å®Ÿéš›ã®ä½¿ç”¨ä¾‹
    print("\nğŸ’¬ ã‚¹ãƒ†ãƒƒãƒ—4: å®Ÿä½¿ç”¨ä¾‹")
    
    demo_conversation = [
        "ã“ã‚“ã«ã¡ã¯ã€åˆã‚ã¾ã—ã¦",
        "ä»Šæ—¥ã¯ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã«ã¤ã„ã¦å­¦ã³ãŸã„ã§ã™",
        "Pythonã‹ã‚‰å§‹ã‚ã‚‹ã®ãŒè‰¯ã„ã§ã—ã‚‡ã†ã‹ï¼Ÿ",
        "ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸ"
    ]
    
    print("ãƒ‡ãƒ¢ä¼šè©±:")
    chatbot.conversation_history = []  # å±¥æ­´ã‚¯ãƒªã‚¢
    
    for user_msg in demo_conversation:
        bot_response = chatbot.generate_response(user_msg, use_history=True)
        print(f"ğŸ‘¤ ãƒ¦ãƒ¼ã‚¶ãƒ¼: {user_msg}")
        print(f"ğŸ¤– ãƒœãƒƒãƒˆ: {bot_response}")
        print()
        
        # ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹æƒ…å ±ã®è¨˜éŒ²
        maintenance.log_interaction(user_msg, bot_response)
    
    # ã‚¹ãƒ†ãƒƒãƒ—5: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç¢ºèª
    print("\nğŸ“Š ã‚¹ãƒ†ãƒƒãƒ—5: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç¢ºèª")
    print(maintenance.generate_usage_report())
    
    # ã‚¹ãƒ†ãƒƒãƒ—6: æ”¹å–„ææ¡ˆ
    print("\nğŸ’¡ ã‚¹ãƒ†ãƒƒãƒ—6: æ”¹å–„ææ¡ˆ")
    improvements = maintenance.suggest_improvements()
    for suggestion in improvements:
        print(f"  â€¢ {suggestion}")
    
    print("\nâœ… å®Ÿè¡Œå®Œäº†!")
    print("ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã¯ä½¿ç”¨æº–å‚™ãŒæ•´ã„ã¾ã—ãŸã€‚")
    
    return chatbot, maintenance

# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ
chatbot_ready, maintenance_system = main_execution_example()
```

## 12. ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚¬ã‚¤ãƒ‰

### ã‚ˆãã‚ã‚‹å•é¡Œã¨è§£æ±ºç­–

```python
class TroubleshootingGuide:
    """
    ã‚ˆãã‚ã‚‹å•é¡Œã®è¨ºæ–­ã¨è§£æ±ºç­–ã‚’æä¾›
    """
    
    @staticmethod
    def diagnose_system():
        """ã‚·ã‚¹ãƒ†ãƒ è¨ºæ–­ã‚’å®Ÿè¡Œ"""
        print("=" * 50)
        print("ã‚·ã‚¹ãƒ†ãƒ è¨ºæ–­é–‹å§‹")
        print("=" * 50)
        
        issues = []
        
        # GPUç¢ºèª
        print("\nğŸ” GPUçŠ¶æ³ç¢ºèª")
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            gpu_allocated = torch.cuda.memory_allocated(0) / 1024**3
            gpu_free = gpu_memory - gpu_allocated
            
            print(f"  âœ… GPUåˆ©ç”¨å¯èƒ½: {gpu_name}")
            print(f"  ğŸ“Š ç·ãƒ¡ãƒ¢ãƒª: {gpu_memory:.1f}GB")
            print(f"  ğŸ“ˆ ä½¿ç”¨ä¸­: {gpu_allocated:.1f}GB")
            print(f"  ğŸ’¾ ç©ºã: {gpu_free:.1f}GB")
            
            if gpu_free < 2.0:
                issues.append("GPUë©”ëª¨ë¦¬ ä¸è¶³ã®å¯èƒ½æ€§")
        else:
            print("  âŒ GPUæœªä½¿ç”¨ï¼ˆCPUå®Ÿè¡Œï¼‰")
            issues.append("GPUæœªä½¿ç”¨ã«ã‚ˆã‚‹æ€§èƒ½ä½ä¸‹")
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç¢ºèª
        print("\nğŸ” ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç¢ºèª")
        try:
            import psutil
            memory = psutil.virtual_memory()
            print(f"  ğŸ“Š ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡: {memory.percent}%")
            print(f"  ğŸ’¾ åˆ©ç”¨å¯èƒ½ãƒ¡ãƒ¢ãƒª: {memory.available / 1024**3:.1f}GB")
            
            if memory.percent > 85:
                issues.append("ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ãŒé«˜ã™ãã¾ã™")
        except ImportError:
            print("  âš ï¸ ãƒ¡ãƒ¢ãƒªç›£è¦–ãƒ„ãƒ¼ãƒ«æœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«")
        
        # ãƒ¢ãƒ‡ãƒ«çŠ¶æ…‹ç¢ºèª
        print("\nğŸ” ãƒ¢ãƒ‡ãƒ«çŠ¶æ…‹ç¢ºèª")
        try:
            model_device = next(model.parameters()).device
            model_dtype = next(model.parameters()).dtype
            model_size = sum(p.numel() for p in model.parameters()) * 4 / 1024**2  # MBæ¦‚ç®—
            
            print(f"  ğŸ“ ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒã‚¤ã‚¹: {model_device}")
            print(f"  ğŸ”¢ ãƒ‡ãƒ¼ã‚¿å‹: {model_dtype}")
            print(f"  ğŸ“ æ¨å®šã‚µã‚¤ã‚º: {model_size:.0f}MB")
            
            if str(model_device) == 'cpu' and torch.cuda.is_available():
                issues.append("ãƒ¢ãƒ‡ãƒ«ãŒGPUã«ç§»å‹•ã•ã‚Œã¦ã„ã¾ã›ã‚“")
                
        except Exception as e:
            issues.append(f"ãƒ¢ãƒ‡ãƒ«çŠ¶æ…‹ç¢ºèªã‚¨ãƒ©ãƒ¼: {e}")
        
        # å•é¡Œãƒ¬ãƒãƒ¼ãƒˆ
        print("\nğŸ“‹ è¨ºæ–­çµæœ")
        if issues:
            print("  ç™ºè¦‹ã•ã‚ŒãŸå•é¡Œ:")
            for issue in issues:
                print(f"    âŒ {issue}")
        else:
            print("  âœ… å•é¡Œã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
        
        return issues
    
    @staticmethod
    def suggest_solutions(issues):
        """å•é¡Œã«å¯¾ã™ã‚‹è§£æ±ºç­–ã‚’ææ¡ˆ"""
        if not issues:
            print("\nâœ¨ ã‚·ã‚¹ãƒ†ãƒ ã¯æ­£å¸¸ã«å‹•ä½œã—ã¦ã„ã¾ã™")
            return
        
        print("\nğŸ”§ æ¨å¥¨è§£æ±ºç­–:")
        
        solutions = {
            "GPUë©”ëª¨ë¦¬ ä¸è¶³": [
                "ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’1ã«å‰Šæ¸›",
                "max_lengthã‚’128ã«çŸ­ç¸®", 
                "å‹¾é…ç´¯ç©ã‚¹ãƒ†ãƒƒãƒ—ã‚’å¢—åŠ ",
                "ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã‚’å†èµ·å‹•ã—ã¦ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢"
            ],
            "GPUæœªä½¿ç”¨": [
                "ãƒ©ãƒ³ã‚¿ã‚¤ãƒ â†’ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã®ã‚¿ã‚¤ãƒ—ã‚’å¤‰æ›´â†’GPUé¸æŠ",
                "model.to(device)ã§GPUã«ç§»å‹•ã‚’ç¢ºèª"
            ],
            "ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡": [
                "ä¸è¦ãªå¤‰æ•°ã‚’delæ–‡ã§å‰Šé™¤",
                "ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œ: import gc; gc.collect()",
                "Colabãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã‚’å†èµ·å‹•"
            ],
            "ãƒ¢ãƒ‡ãƒ«ãŒGPUã«ç§»å‹•ã•ã‚Œã¦ã„ãªã„": [
                "model.to(device)ã‚’å†å®Ÿè¡Œ",
                "deviceè¨­å®šã‚’ç¢ºèª"
            ]
        }
        
        for issue in issues:
            for problem_type, solution_list in solutions.items():
                if problem_type in issue:
                    print(f"\nğŸ”¸ {issue}")
                    for solution in solution_list:
                        print(f"    ğŸ’¡ {solution}")
                    break
    
    @staticmethod
    def performance_optimization():
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã®ãƒ’ãƒ³ãƒˆ"""
        print("\nâš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã®ãƒ’ãƒ³ãƒˆ")
        
        tips = [
            "å­¦ç¿’æ™‚ã¯model.train()ã€æ¨è«–æ™‚ã¯model.eval()ã‚’ç¢ºå®Ÿã«è¨­å®š",
            "æ¨è«–æ™‚ã¯with torch.no_grad():ã‚’ä½¿ç”¨ã—ã¦ãƒ¡ãƒ¢ãƒªç¯€ç´„",
            "ãƒãƒƒãƒã‚µã‚¤ã‚ºã¯å°ã•ãã€å‹¾é…ç´¯ç©ã§å®ŸåŠ¹ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’èª¿æ•´",
            "å®šæœŸçš„ã«torch.cuda.empty_cache()ã§GPUãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªã‚¢",
            "é•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã¯é¿ã‘ã€é©åˆ‡ãªmax_lengthã‚’è¨­å®š",
            "FP16ã‚’ä½¿ç”¨ã—ã¦ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’åŠæ¸›",
            "ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®num_workersã‚’èª¿æ•´",
            "ä¸è¦ãªå‡ºåŠ›ã‚„ãƒ­ã‚°ã‚’å‰Šæ¸›"
        ]
        
        for i, tip in enumerate(tips, 1):
            print(f"  {i}. {tip}")

# è¨ºæ–­å®Ÿè¡Œ
troubleshooter = TroubleshootingGuide()
detected_issues = troubleshooter.diagnose_system()
troubleshooter.suggest_solutions(detected_issues)
troubleshooter.performance_optimization()
```

## 13. å®Ÿç”¨çš„ãªãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°

### ä¾¿åˆ©ãªè£œåŠ©æ©Ÿèƒ½

```python
class ChatBotUtils:
    """
    ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆé–‹ç™ºã«å½¹ç«‹ã¤ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°é›†
    """
    
    @staticmethod
    def save_conversation_history(history: List[Dict], filename: str = None):
        """ä¼šè©±å±¥æ­´ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"conversation_history_{timestamp}.json"
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(history, f, ensure_ascii=False, indent=2)
            print(f"ä¼šè©±å±¥æ­´ã‚’ {filename} ã«ä¿å­˜ã—ã¾ã—ãŸ")
            return filename
        except Exception as e:
            print(f"ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    @staticmethod
    def load_conversation_history(filename: str) -> List[Dict]:
        """JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ä¼šè©±å±¥æ­´ã‚’èª­ã¿è¾¼ã¿"""
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                history = json.load(f)
            print(f"ä¼šè©±å±¥æ­´ã‚’ {filename} ã‹ã‚‰èª­ã¿è¾¼ã¿ã¾ã—ãŸ ({len(history)}ä»¶)")
            return history
        except Exception as e:
            print(f"èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    @staticmethod
    def analyze_conversation_patterns(history: List[Dict]):
        """ä¼šè©±ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æ"""
        if not history:
            print("åˆ†æã™ã‚‹ä¼šè©±å±¥æ­´ãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        print("\nğŸ“Š ä¼šè©±ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ")
        print("-" * 30)
        
        # åŸºæœ¬çµ±è¨ˆ
        total_turns = len(history)
        avg_user_length = sum(len(turn['user']) for turn in history) / total_turns
        avg_bot_length = sum(len(turn['bot']) for turn in history) / total_turns
        
        print(f"ç·ã‚¿ãƒ¼ãƒ³æ•°: {total_turns}")
        print(f"ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›å¹³å‡é•·: {avg_user_length:.1f}æ–‡å­—")
        print(f"ãƒœãƒƒãƒˆå¿œç­”å¹³å‡é•·: {avg_bot_length:.1f}æ–‡å­—")
        
        # ã‚ˆãä½¿ã‚ã‚Œã‚‹å˜èª
        user_words = []
        bot_words = []
        
        for turn in history:
            user_words.extend(turn['user'].split())
            bot_words.extend(turn['bot'].split())
        
        from collections import Counter
        
        user_common = Counter(user_words).most_common(5)
        bot_common = Counter(bot_words).most_common(5)
        
        print("\nãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆãä½¿ã‚ã‚Œã‚‹å˜èª:")
        for word, count in user_common:
            print(f"  {word}: {count}å›")
        
        print("\nãƒœãƒƒãƒˆå¿œç­”ã«ã‚ˆãå«ã¾ã‚Œã‚‹å˜èª:")
        for word, count in bot_common:
            print(f"  {word}: {count}å›")
    
    @staticmethod
    def benchmark_response_time(chatbot, test_inputs: List[str], runs: int = 3):
        """å¿œç­”æ™‚é–“ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        print(f"\nâ±ï¸ å¿œç­”æ™‚é–“ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ ({runs}å›å®Ÿè¡Œ)")
        print("-" * 40)
        
        import time
        
        results = {}
        
        for test_input in test_inputs:
            times = []
            
            for run in range(runs):
                start_time = time.time()
                response = chatbot.generate_response(test_input, use_history=False)
                end_time = time.time()
                times.append(end_time - start_time)
            
            avg_time = sum(times) / len(times)
            min_time = min(times)
            max_time = max(times)
            
            results[test_input] = {
                'average': avg_time,
                'min': min_time,
                'max': max_time,
                'response_length': len(response)
            }
            
            print(f"'{test_input[:20]}...':")
            print(f"  å¹³å‡: {avg_time:.2f}s")
            print(f"  æœ€å°: {min_time:.2f}s") 
            print(f"  æœ€å¤§: {max_time:.2f}s")
            print(f"  å¿œç­”é•·: {len(response)}æ–‡å­—")
            print()
        
        # å…¨ä½“çµ±è¨ˆ
        all_avg_times = [r['average'] for r in results.values()]
        overall_avg = sum(all_avg_times) / len(all_avg_times)
        print(f"å…¨ä½“å¹³å‡å¿œç­”æ™‚é–“: {overall_avg:.2f}ç§’")
        
        return results
    
    @staticmethod
    def export_model_info(model, tokenizer, filepath: str = "model_info.json"):
        """ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        try:
            model_info = {
                'model_name': model.config.name_or_path if hasattr(model.config, 'name_or_path') else 'unknown',
                'model_type': model.config.model_type if hasattr(model.config, 'model_type') else 'unknown',
                'vocab_size': tokenizer.vocab_size,
                'max_position_embeddings': model.config.max_position_embeddings if hasattr(model.config, 'max_position_embeddings') else 'unknown',
                'num_parameters': sum(p.numel() for p in model.parameters()),
                'trainable_parameters': sum(p.numel() for p in model.parameters() if p.requires_grad),
                'device': str(next(model.parameters()).device),
                'dtype': str(next(model.parameters()).dtype),
                'export_timestamp': datetime.now().isoformat()
            }
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(model_info, f, indent=2, ensure_ascii=False)
            
            print(f"ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’ {filepath} ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ã¾ã—ãŸ")
            
            # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«ã‚‚è¡¨ç¤º
            print("\nğŸ“‹ ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚µãƒãƒªãƒ¼:")
            for key, value in model_info.items():
                if key != 'export_timestamp':
                    print(f"  {key}: {value:,}" if isinstance(value, int) else f"  {key}: {value}")
            
            return model_info
            
        except Exception as e:
            print(f"ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    @staticmethod
    def memory_cleanup():
        """ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        print("ğŸ§¹ ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Ÿè¡Œä¸­...")
        
        import gc
        
        # Python ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
        collected = gc.collect()
        print(f"  Python GC: {collected}ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆè§£æ”¾")
        
        # PyTorch GPU ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            allocated = torch.cuda.memory_allocated(0) / 1024**3
            print(f"  GPU ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢å®Œäº†")
            print(f"  ç¾åœ¨ã®GPUä½¿ç”¨é‡: {allocated:.2f}GB")
        
        print("âœ… ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†")

# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã®ä½¿ç”¨ä¾‹
utils = ChatBotUtils()

# ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
model_info = utils.export_model_info(model, tokenizer)

# ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆ
benchmark_inputs = [
    "ã“ã‚“ã«ã¡ã¯",
    "ä»Šæ—¥ã¯è‰¯ã„å¤©æ°—ã§ã™ã­",
    "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã«ã¤ã„ã¦æ•™ãˆã¦",
    "ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™"
]

benchmark_results = utils.benchmark_response_time(chatbot, benchmark_inputs)

# ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
utils.memory_cleanup()
```

## 14. æœ€çµ‚ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆã¨é‹ç”¨é–‹å§‹

### é‹ç”¨å‰æœ€çµ‚ãƒã‚§ãƒƒã‚¯

```python
def final_system_check():
    """
    é‹ç”¨é–‹å§‹å‰ã®æœ€çµ‚ã‚·ã‚¹ãƒ†ãƒ ãƒã‚§ãƒƒã‚¯
    """
    
    print("=" * 60)
    print("ğŸ” æœ€çµ‚ã‚·ã‚¹ãƒ†ãƒ ãƒã‚§ãƒƒã‚¯")
    print("=" * 60)
    
    checks = []
    
    # 1. ãƒ¢ãƒ‡ãƒ«å‹•ä½œç¢ºèª
    print("\n1ï¸âƒ£ ãƒ¢ãƒ‡ãƒ«å‹•ä½œç¢ºèª")
    try:
        test_response = chatbot.generate_response("ã‚·ã‚¹ãƒ†ãƒ ãƒã‚§ãƒƒã‚¯")
        if test_response and len(test_response) > 5:
            print("  âœ… ãƒ¢ãƒ‡ãƒ«æ­£å¸¸å‹•ä½œ")
            checks.append(True)
        else:
            print("  âŒ ãƒ¢ãƒ‡ãƒ«å¿œç­”ç•°å¸¸")
            checks.append(False)
    except Exception as e:
        print(f"  âŒ ãƒ¢ãƒ‡ãƒ«å‹•ä½œã‚¨ãƒ©ãƒ¼: {e}")
        checks.append(False)
    
    # 2. GPUä½¿ç”¨ç¢ºèª
    print("\n2ï¸âƒ£ GPUä½¿ç”¨ç¢ºèª")
    if torch.cuda.is_available() and next(model.parameters()).is_cuda:
        print("  âœ… GPUä½¿ç”¨ä¸­")
        checks.append(True)
    else:
        print("  âš ï¸ CPUä½¿ç”¨ï¼ˆæ€§èƒ½ä½ä¸‹ã®å¯èƒ½æ€§ï¼‰")
        checks.append(False)
    
    # 3. ãƒ¡ãƒ¢ãƒªçŠ¶æ…‹ç¢ºèª
    print("\n3ï¸âƒ£ ãƒ¡ãƒ¢ãƒªçŠ¶æ…‹ç¢ºèª")
    if torch.cuda.is_available():
        memory_usage = torch.cuda.memory_allocated(0) / torch.cuda.max_memory_allocated(0) * 100
        if memory_usage < 80:
            print(f"  âœ… GPUë©”ëª¨ë¦¬ ä½¿ç”¨ç‡: {memory_usage:.1f}%")
            checks.append(True)
        else:
            print(f"  âš ï¸ GPUë©”ëª¨ë¦¬ ä½¿ç”¨ç‡é«˜: {memory_usage:.1f}%")
            checks.append(False)
    else:
        print("  âš ï¸ GPUë©”ëª¨ë¦¬ ç¢ºèªä¸å¯")
        checks.append(False)
    
    # 4. å¿œç­”å“è³ªç¢ºèª
    print("\n4ï¸âƒ£ å¿œç­”å“è³ªç¢ºèª")
    quality_tests = [
        ("æŒ¨æ‹¶ãƒ†ã‚¹ãƒˆ", "ã“ã‚“ã«ã¡ã¯"),
        ("è³ªå•å¿œç­”ãƒ†ã‚¹ãƒˆ", "å…ƒæ°—ã§ã™ã‹ï¼Ÿ"),
        ("æ„Ÿè¬å¿œç­”ãƒ†ã‚¹ãƒˆ", "ã‚ã‚ŠãŒã¨ã†")
    ]
    
    quality_scores = []
    for test_name, test_input in quality_tests:
        response = chatbot.generate_response(test_input, use_history=False)
        # ç°¡æ˜“å“è³ªè©•ä¾¡
        if response and 10 <= len(response) <= 150 and "ã‚¨ãƒ©ãƒ¼" not in response:
            quality_scores.append(1)
            print(f"  âœ… {test_name}: åˆæ ¼")
        else:
            quality_scores.append(0)
            print(f"  âŒ {test_name}: ä¸åˆæ ¼")
    
    quality_rate = sum(quality_scores) / len(quality_scores)
    if quality_rate >= 0.7:
        print(f"  âœ… å¿œç­”å“è³ª: {quality_rate*100:.0f}%")
        checks.append(True)
    else:
        print(f"  âŒ å¿œç­”å“è³ªä¸è¶³: {quality_rate*100:.0f}%")
        checks.append(False)
    
    # 5. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ç¢ºèª
    print("\n5ï¸âƒ£ ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ç¢ºèª")
    try:
        error_response = chatbot.generate_response("", use_history=False)
        if error_response:
            print("  âœ… ã‚¨ãƒ©ãƒ¼å‡¦ç†æ­£å¸¸")
            checks.append(True)
        else:
            print("  âŒ ã‚¨ãƒ©ãƒ¼å‡¦ç†ç•°å¸¸")
            checks.append(False)
    except Exception as e:
        print(f"  âŒ ã‚¨ãƒ©ãƒ¼å‡¦ç†å¤±æ•—: {e}")
        checks.append(False)
    
    # æœ€çµ‚çµæœ
    print("\n" + "=" * 60)
    passed_checks = sum(checks)
    total_checks = len(checks)
    success_rate = passed_checks / total_checks * 100
    
    print(f"ğŸ“Š æœ€çµ‚çµæœ: {passed_checks}/{total_checks} é …ç›®åˆæ ¼ ({success_rate:.0f}%)")
    
    if success_rate >= 80:
        print("ğŸ‰ ã‚·ã‚¹ãƒ†ãƒ é‹ç”¨æº–å‚™å®Œäº†ï¼")
        status = "READY"
    elif success_rate >= 60:
        print("âš ï¸ ä¸€éƒ¨å•é¡ŒãŒã‚ã‚Šã¾ã™ãŒä½¿ç”¨å¯èƒ½ã§ã™")
        status = "CAUTION"
    else:
        print("âŒ é‡å¤§ãªå•é¡ŒãŒã‚ã‚Šã¾ã™ã€‚ä¿®æ­£ãŒå¿…è¦ã§ã™")
        status = "ERROR"
    
    return status, checks

# æœ€çµ‚ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ
system_status, check_results = final_system_check()

# é‹ç”¨é–‹å§‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
if system_status == "READY":
    print("\n" + "=" * 60)
    print("ğŸš€ GPT-2ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆé‹ç”¨é–‹å§‹ï¼")
    print("=" * 60)
    print()
    print("åˆ©ç”¨å¯èƒ½ãªæ©Ÿèƒ½:")
    print("  â€¢ chatbot.interactive_chat() - ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒãƒ£ãƒƒãƒˆ")
    print("  â€¢ chatbot.generate_response(text) - å˜ç™ºå¿œç­”ç”Ÿæˆ")
    print("  â€¢ evaluator.evaluate_comprehensive() - æ€§èƒ½è©•ä¾¡")
    print("  â€¢ maintenance.generate_usage_report() - ä½¿ç”¨çŠ¶æ³å ±å‘Š")
    print("  â€¢ utils.memory_cleanup() - ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—")
    print()
    print("ä½¿ç”¨ä¾‹:")
    print("  chatbot.interactive_chat()  # ãƒãƒ£ãƒƒãƒˆé–‹å§‹")
    print()
    print("æ¥½ã—ã„ãƒãƒ£ãƒƒãƒˆã‚’ãŠæ¥½ã—ã¿ãã ã•ã„ï¼ ğŸ¤–âœ¨")
    
elif system_status == "CAUTION":
    print("\nâš ï¸ æ³¨æ„äº‹é …:")
    print("  â€¢ å¿œç­”é€Ÿåº¦ãŒé…ã„å ´åˆãŒã‚ã‚Šã¾ã™")
    print("  â€¢ ãƒ¡ãƒ¢ãƒªä¸è¶³ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™") 
    print("  â€¢ å®šæœŸçš„ã«utils.memory_cleanup()ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
    
else:
    print("\nâŒ ä¿®æ­£ãŒå¿…è¦ãªé …ç›®:")
    print("  â€¢ GPUè¨­å®šã®ç¢ºèª")
    print("  â€¢ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã®å†å®Ÿè¡Œ")
    print("  â€¢ ãƒ¡ãƒ¢ãƒªä¸è¶³ã®è§£æ¶ˆ")
```

## 15. ã¾ã¨ã‚ã¨ä»Šå¾Œã®ç™ºå±•

### ã“ã®ã‚¬ã‚¤ãƒ‰ã§å­¦ã‚“ã ã“ã¨

ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€ä»¥ä¸‹ã®é‡è¦ãªè¦ç´ ã‚’ã‚«ãƒãƒ¼ã—ã¾ã—ãŸï¼š

**åŸºç¤æŠ€è¡“**
- GPT-2ãƒ¢ãƒ‡ãƒ«ã®ç†è§£ã¨æ´»ç”¨
- Hugging Face Transformersãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ä½¿ç”¨
- Google Colabç’°å¢ƒã§ã®åŠ¹ç‡çš„ãªé–‹ç™º

**å®Ÿè£…æŠ€è¡“**
- é«˜å“è³ªãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­è¨ˆ
- åŠ¹æœçš„ãªãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ‰‹æ³•
- å¿œç­”ç”Ÿæˆã®æœ€é©åŒ–
- ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°

**é‹ç”¨æŠ€è¡“**
- æ€§èƒ½è©•ä¾¡ã¨å“è³ªç®¡ç†
- ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ã¨ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°
- ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°
- ç¶™ç¶šçš„æ”¹å–„

### ä»Šå¾Œã®ç™ºå±•å¯èƒ½æ€§

1. **ã‚ˆã‚Šå¤§ããªãƒ¢ãƒ‡ãƒ«ã¸ã®ç§»è¡Œ**
   - GPT-2 medium/large
   - GPT-3.5ã‚„GPT-4ç­‰ã®æœ€æ–°ãƒ¢ãƒ‡ãƒ«

2. **å°‚é–€åˆ†é‡ã¸ã®ç‰¹åŒ–**
   - ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆ
   - æ•™è‚²æ”¯æ´
   - å‰µä½œæ”¯æ´

3. **å¤šè¨€èªå¯¾å¿œ**
   - è‹±èªã‚„ãã®ä»–è¨€èªã¸ã®å±•é–‹
   - å¤šè¨€èªé–“ç¿»è¨³æ©Ÿèƒ½

4. **å¤–éƒ¨ã‚·ã‚¹ãƒ†ãƒ é€£æº**
   - APIçµ±åˆ
   - ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹é€£æº
   - ã‚¦ã‚§ãƒ–ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³åŒ–

ã“ã®ã‚¬ã‚¤ãƒ‰ãŒã€ã‚ãªãŸã®ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆé–‹ç™ºã®æˆåŠŸã«ã¤ãªãŒã‚‹ã“ã¨ã‚’é¡˜ã£ã¦ã„ã¾ã™ã€‚ç¶™ç¶šçš„ãªå­¦ç¿’ã¨æ”¹å–„ã‚’é€šã˜ã¦ã€ã‚ˆã‚Šå„ªã‚ŒãŸAIã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã—ã¦ãã ã•ã„ï¼

---

**æœ€çµ‚æ›´æ–°**: 2025å¹´8æœˆç‰ˆ
**å¯¾å¿œç’°å¢ƒ**: Google Colab, Python 3.7+, PyTorch 1.12+
**ãƒ©ã‚¤ã‚»ãƒ³ã‚¹**: æ•™è‚²ãƒ»ç ”ç©¶ç›®çš„ã§ã®è‡ªç”±åˆ©ç”¨å¯
    