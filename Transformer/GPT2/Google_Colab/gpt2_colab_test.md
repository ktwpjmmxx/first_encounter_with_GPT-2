# Google Colab ã§GPT-2ã‚’ä½¿ç”¨ã—ãŸæ–‡ç« ç”Ÿæˆãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«

ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã§ã¯ã€Hugging Face ã®äº‹å‰å­¦ç¿’æ¸ˆã¿GPT-2ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã€Google Colabä¸Šã§æ–‡ç« ç”Ÿæˆã‚’è¡Œã†æ–¹æ³•ã‚’è§£èª¬ã—ã¾ã™ã€‚

## 1. ç’°å¢ƒè¨­å®šã¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

### 1.1 å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```python
# Google Colabã§æœ€åˆã«å®Ÿè¡Œã™ã‚‹ã‚»ãƒ«

# transformersãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆæœ€æ–°ç‰ˆï¼‰
!pip install transformers>=4.30.0

# ãã®ä»–ã®å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒª
!pip install torch>=2.0.0 torchvision torchaudio

# ã‚ªãƒ—ã‚·ãƒ§ãƒ³: ã‚ˆã‚Šé«˜é€Ÿãªæ¨è«–ã®ãŸã‚ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
!pip install accelerate>=0.20.0

print("ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†ï¼")
```

### 1.2 ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ

```python
# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer
import warnings
warnings.filterwarnings('ignore')

# Google Colabç’°å¢ƒã®ç¢ºèª
print("=== ç’°å¢ƒæƒ…å ± ===")
print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU device: {torch.cuda.get_device_name(0)}")
    print(f"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
else:
    print("CPUã‚’ä½¿ç”¨ã—ã¾ã™")
```

## 2. ãƒ¢ãƒ‡ãƒ«ã¨Tokenizerã®èª­ã¿è¾¼ã¿

### 2.1 äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿

```python
# ãƒ¢ãƒ‡ãƒ«åã®æŒ‡å®šï¼ˆåˆ©ç”¨å¯èƒ½ãªGPT-2ã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
model_names = {
    'gpt2': 'gpt2',                    # 117M parameters (æœ€å°)
    'gpt2-medium': 'gpt2-medium',      # 345M parameters
    'gpt2-large': 'gpt2-large',        # 762M parameters  
    'gpt2-xl': 'gpt2-xl'               # 1.5B parameters (æœ€å¤§)
}

# ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠï¼ˆãƒ¡ãƒ¢ãƒªåˆ¶é™ã«å¿œã˜ã¦èª¿æ•´ï¼‰
model_name = 'gpt2'  # Google Colabç„¡æ–™ç‰ˆã§ã¯ 'gpt2' ã¾ãŸã¯ 'gpt2-medium' ã‚’æ¨å¥¨

print(f"ãƒ¢ãƒ‡ãƒ« '{model_name}' ã‚’èª­ã¿è¾¼ã¿ä¸­...")

# Tokenizerã®èª­ã¿è¾¼ã¿
tokenizer = GPT2Tokenizer.from_pretrained(model_name)

# ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
model = GPT2LMHeadModel.from_pretrained(model_name)

# ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ã®è¨­å®šï¼ˆGPT-2ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ãŒãªã„ï¼‰
tokenizer.pad_token = tokenizer.eos_token

print("ãƒ¢ãƒ‡ãƒ«ã¨Tokenizerã®èª­ã¿è¾¼ã¿å®Œäº†ï¼")
print(f"èªå½™ã‚µã‚¤ã‚º: {tokenizer.vocab_size}")
print(f"ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {model.num_parameters():,}")
```

### 2.2 GPUã¸ã®ç§»å‹•ï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰

```python
# ãƒ‡ãƒã‚¤ã‚¹ã®è¨­å®š
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)

print(f"ãƒ‡ãƒã‚¤ã‚¹: {device}")

# GPUä½¿ç”¨æ™‚ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç¢ºèª
if torch.cuda.is_available():
    print(f"GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {torch.cuda.memory_allocated() / 1e9:.2f} GB")
```

## 3. åŸºæœ¬çš„ãªæ–‡ç« ç”Ÿæˆ

### 3.1 ã‚·ãƒ³ãƒ—ãƒ«ãªç”Ÿæˆé–¢æ•°

```python
def generate_text(prompt, max_length=100, temperature=0.8, top_p=0.9, num_return_sequences=1):
    """
    GPT-2ã‚’ä½¿ç”¨ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°
    
    Args:
        prompt (str): å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        max_length (int): ç”Ÿæˆã™ã‚‹æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
        temperature (float): ç”Ÿæˆã®ãƒ©ãƒ³ãƒ€ãƒ æ€§ï¼ˆé«˜ã„ã»ã©ã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ï¼‰
        top_p (float): Nucleus sampling ã®é–¾å€¤
        num_return_sequences (int): ç”Ÿæˆã™ã‚‹ã‚·ãƒ¼ã‚±ãƒ³ã‚¹æ•°
    
    Returns:
        list: ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
    """
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)
    
    print(f"å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: '{prompt}'")
    print(f"å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {input_ids.shape[1]}")
    print(f"ç”Ÿæˆè¨­å®š: max_length={max_length}, temperature={temperature}, top_p={top_p}")
    print("=" * 50)
    
    # ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®š
    model.eval()
    
    # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆå‹¾é…è¨ˆç®—ã‚’ç„¡åŠ¹åŒ–ã—ã¦ãƒ¡ãƒ¢ãƒªç¯€ç´„ï¼‰
    with torch.no_grad():
        output_sequences = model.generate(
            input_ids,
            max_length=max_length,
            temperature=temperature,
            top_p=top_p,
            num_return_sequences=num_return_sequences,
            do_sample=True,               # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’æœ‰åŠ¹åŒ–
            pad_token_id=tokenizer.eos_token_id,
            repetition_penalty=1.1        # ç¹°ã‚Šè¿”ã—ã‚’é˜²ã
        )
    
    # ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
    generated_texts = []
    for i, sequence in enumerate(output_sequences):
        # å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ã‚’é™¤å»
        generated_sequence = sequence[input_ids.shape[1]:]
        text = tokenizer.decode(generated_sequence, skip_special_tokens=True)
        generated_texts.append(text)
        
        print(f"ç”Ÿæˆçµæœ {i+1}:")
        print(f"'{text}'")
        print("-" * 30)
    
    return generated_texts

# ä½¿ç”¨ä¾‹
prompt = "Once upon a time"
results = generate_text(prompt, max_length=50)
```

### 3.2 æ—¥æœ¬èªå¯¾å¿œç‰ˆï¼ˆå¤šè¨€èªãƒ¢ãƒ‡ãƒ«ä½¿ç”¨ï¼‰

```python
# æ—¥æœ¬èªå¯¾å¿œã®GPT-2ãƒ¢ãƒ‡ãƒ«ï¼ˆrinnaç¤¾ãªã©ï¼‰
# æ³¨æ„: æ—¥æœ¬èªç”¨ã®äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã¯åˆ¥é€”ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒå¿…è¦

def setup_japanese_gpt2():
    """æ—¥æœ¬èªGPT-2ãƒ¢ãƒ‡ãƒ«ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰"""
    try:
        # rinnaç¤¾ã®æ—¥æœ¬èªGPT-2ãƒ¢ãƒ‡ãƒ«ä¾‹
        # !pip install fugashi ipadic
        
        from transformers import T5Tokenizer, AutoModelForCausalLM
        
        model_name = "rinna/japanese-gpt2-medium"
        
        tokenizer_ja = T5Tokenizer.from_pretrained(model_name)
        model_ja = AutoModelForCausalLM.from_pretrained(model_name)
        
        return tokenizer_ja, model_ja
    except:
        print("æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚è‹±èªãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚")
        return None, None

# æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã®ä½¿ç”¨ä¾‹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
# tokenizer_ja, model_ja = setup_japanese_gpt2()
```

## 4. é«˜åº¦ãªç”Ÿæˆè¨­å®šã¨ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º

### 4.1 ç•°ãªã‚‹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ‰‹æ³•ã®æ¯”è¼ƒ

```python
def compare_sampling_methods(prompt, max_length=80):
    """ç•°ãªã‚‹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ‰‹æ³•ã‚’æ¯”è¼ƒ"""
    
    print(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: '{prompt}'")
    print("=" * 60)
    
    # 1. Greedy Decodingï¼ˆæ±ºå®šçš„ï¼‰
    print("1. Greedy Decodingï¼ˆæœ€ã‚‚ç¢ºç‡ã®é«˜ã„ãƒˆãƒ¼ã‚¯ãƒ³ã‚’é¸æŠï¼‰")
    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)
    
    with torch.no_grad():
        greedy_output = model.generate(
            input_ids,
            max_length=max_length,
            do_sample=False,  # Greedyãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
            pad_token_id=tokenizer.eos_token_id
        )
    
    greedy_text = tokenizer.decode(greedy_output[0][input_ids.shape[1]:], skip_special_tokens=True)
    print(f"çµæœ: {greedy_text}")
    print()
    
    # 2. Temperature Sampling
    print("2. Temperature Samplingï¼ˆæ¸©åº¦åˆ¶å¾¡ï¼‰")
    temperatures = [0.3, 0.8, 1.2]
    
    for temp in temperatures:
        with torch.no_grad():
            temp_output = model.generate(
                input_ids,
                max_length=max_length,
                temperature=temp,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        temp_text = tokenizer.decode(temp_output[0][input_ids.shape[1]:], skip_special_tokens=True)
        print(f"  Temperature {temp}: {temp_text}")
    print()
    
    # 3. Top-p (Nucleus) Sampling
    print("3. Top-p Samplingï¼ˆæ ¸ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰")
    top_p_values = [0.5, 0.8, 0.95]
    
    for p in top_p_values:
        with torch.no_grad():
            nucleus_output = model.generate(
                input_ids,
                max_length=max_length,
                temperature=0.8,
                top_p=p,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        nucleus_text = tokenizer.decode(nucleus_output[0][input_ids.shape[1]:], skip_special_tokens=True)
        print(f"  Top-p {p}: {nucleus_text}")
    print()

# ä½¿ç”¨ä¾‹
compare_sampling_methods("The future of artificial intelligence")
```

### 4.2 ãƒãƒƒãƒå‡¦ç†ã§ã®è¤‡æ•°ç”Ÿæˆ

```python
def batch_generate(prompts, max_length=60, batch_size=4):
    """è¤‡æ•°ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’åŠ¹ç‡çš„ã«ãƒãƒƒãƒå‡¦ç†"""
    
    print(f"ãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_size}")
    print(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ•°: {len(prompts)}")
    print("=" * 50)
    
    all_results = []
    
    # ãƒãƒƒãƒå˜ä½ã§å‡¦ç†
    for i in range(0, len(prompts), batch_size):
        batch_prompts = prompts[i:i+batch_size]
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
        inputs = tokenizer(batch_prompts, return_tensors='pt', padding=True, truncation=True)
        input_ids = inputs['input_ids'].to(device)
        attention_mask = inputs['attention_mask'].to(device)
        
        with torch.no_grad():
            outputs = model.generate(
                input_ids,
                attention_mask=attention_mask,
                max_length=max_length,
                temperature=0.8,
                top_p=0.9,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã¦çµæœã‚’ä¿å­˜
        for j, output in enumerate(outputs):
            original_length = input_ids[j].shape[0]
            generated = output[original_length:]
            text = tokenizer.decode(generated, skip_special_tokens=True)
            
            print(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ {i+j+1}: '{batch_prompts[j]}'")
            print(f"ç”Ÿæˆçµæœ: {text}")
            print("-" * 30)
            
            all_results.append(text)
    
    return all_results

# ä½¿ç”¨ä¾‹
test_prompts = [
    "The weather today is",
    "In the distant future",
    "My favorite hobby is",
    "The secret to happiness"
]

batch_results = batch_generate(test_prompts)
```

## 5. å®Ÿç”¨çš„ãªå¿œç”¨ä¾‹

### 5.1 ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ç”Ÿæˆ

```python
def generate_story(initial_sentence, num_paragraphs=3, sentences_per_paragraph=3):
    """æ®µè½æ§‹æˆã®ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã‚’ç”Ÿæˆ"""
    
    print(f"ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã®é–‹å§‹: '{initial_sentence}'")
    print("=" * 60)
    
    current_text = initial_sentence
    story_parts = [initial_sentence]
    
    for paragraph in range(num_paragraphs):
        print(f"\næ®µè½ {paragraph + 1}:")
        paragraph_text = ""
        
        for sentence in range(sentences_per_paragraph):
            # ç¾åœ¨ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å…ƒã«æ¬¡ã®æ–‡ã‚’ç”Ÿæˆ
            input_ids = tokenizer.encode(current_text, return_tensors='pt').to(device)
            
            with torch.no_grad():
                output = model.generate(
                    input_ids,
                    max_length=input_ids.shape[1] + 30,  # é©åº¦ãªé•·ã•
                    temperature=0.8,
                    top_p=0.9,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id
                )
            
            # æ–°ã—ã„éƒ¨åˆ†ã®ã¿æŠ½å‡º
            new_tokens = output[0][input_ids.shape[1]:]
            new_text = tokenizer.decode(new_tokens, skip_special_tokens=True)
            
            # æœ€åˆã®æ–‡ã‚’å–å¾—ï¼ˆå¥ç‚¹ã§åŒºåˆ‡ã‚Šï¼‰
            sentences = new_text.split('.')
            if len(sentences) > 1:
                new_sentence = sentences[0] + '.'
            else:
                new_sentence = new_text
            
            paragraph_text += " " + new_sentence
            current_text += " " + new_sentence
        
        story_parts.append(paragraph_text.strip())
        print(paragraph_text.strip())
    
    return story_parts

# ä½¿ç”¨ä¾‹
story = generate_story("In a small village nestled between mountains")
```

### 5.2 è³ªå•å¿œç­”å½¢å¼

```python
def qa_generation(context, question):
    """æ–‡è„ˆã«åŸºã¥ãè³ªå•å¿œç­”"""
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®ä½œæˆ
    prompt = f"Context: {context}\nQuestion: {question}\nAnswer:"
    
    print(f"æ–‡è„ˆ: {context}")
    print(f"è³ªå•: {question}")
    print("-" * 40)
    
    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)
    
    with torch.no_grad():
        output = model.generate(
            input_ids,
            max_length=input_ids.shape[1] + 50,
            temperature=0.3,  # ã‚ˆã‚Šç¢ºå®Ÿãªå›ç­”ã®ãŸã‚ä½ã‚ã«è¨­å®š
            top_p=0.8,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.encode('\n')[0]  # æ”¹è¡Œã§åœæ­¢
        )
    
    # å›ç­”éƒ¨åˆ†ã®ã¿æŠ½å‡º
    answer_tokens = output[0][input_ids.shape[1]:]
    answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)
    
    print(f"å›ç­”: {answer}")
    return answer

# ä½¿ç”¨ä¾‹
context = "The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France."
question = "Where is the Eiffel Tower located?"
qa_generation(context, question)
```

## 6. ãƒ¡ãƒ¢ãƒªç®¡ç†ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

### 6.1 ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®ç›£è¦–

```python
def monitor_memory():
    """GPU/CPUãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®ç›£è¦–"""
    
    import psutil
    
    print("=== ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ ===")
    
    # CPU RAM
    ram = psutil.virtual_memory()
    print(f"RAMä½¿ç”¨é‡: {ram.used / 1e9:.1f} GB / {ram.total / 1e9:.1f} GB ({ram.percent:.1f}%)")
    
    # GPU ãƒ¡ãƒ¢ãƒªï¼ˆCUDAåˆ©ç”¨å¯èƒ½æ™‚ï¼‰
    if torch.cuda.is_available():
        gpu_memory = torch.cuda.memory_allocated()
        gpu_reserved = torch.cuda.memory_reserved()
        gpu_total = torch.cuda.get_device_properties(0).total_memory
        
        print(f"GPUä½¿ç”¨é‡: {gpu_memory / 1e9:.1f} GB")
        print(f"GPUäºˆç´„é‡: {gpu_reserved / 1e9:.1f} GB") 
        print(f"GPUç·å®¹é‡: {gpu_total / 1e9:.1f} GB")
        print(f"GPUä½¿ç”¨ç‡: {(gpu_memory / gpu_total) * 100:.1f}%")

# ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—é–¢æ•°
def cleanup_memory():
    """ãƒ¡ãƒ¢ãƒªã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
    
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    import gc
    gc.collect()
    
    print("ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†")

# ä½¿ç”¨ä¾‹
monitor_memory()
```

### 6.2 åŠ¹ç‡çš„ãªç”Ÿæˆè¨­å®š

```python
def optimized_generation(prompt, max_length=100):
    """ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã‚’è€ƒæ…®ã—ãŸç”Ÿæˆ"""
    
    # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã®ãŸã‚ã®è¨­å®š
    generation_config = {
        'max_length': max_length,
        'temperature': 0.8,
        'top_p': 0.9,
        'do_sample': True,
        'pad_token_id': tokenizer.eos_token_id,
        'use_cache': True,  # KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ç”¨
        'output_scores': False,  # ã‚¹ã‚³ã‚¢å‡ºåŠ›ã‚’ç„¡åŠ¹åŒ–ã—ã¦ãƒ¡ãƒ¢ãƒªç¯€ç´„
        'return_dict_in_generate': False
    }
    
    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)
    
    # è‡ªå‹•æ··åˆç²¾åº¦ã‚’ä½¿ç”¨ï¼ˆGPUåˆ©ç”¨æ™‚ï¼‰
    if torch.cuda.is_available():
        with torch.cuda.amp.autocast():
            with torch.no_grad():
                output = model.generate(input_ids, **generation_config)
    else:
        with torch.no_grad():
            output = model.generate(input_ids, **generation_config)
    
    # çµæœã®ãƒ‡ã‚³ãƒ¼ãƒ‰
    generated_text = tokenizer.decode(output[0][input_ids.shape[1]:], skip_special_tokens=True)
    
    return generated_text

# ä½¿ç”¨ä¾‹
result = optimized_generation("The benefits of renewable energy include")
print(result)
```

## 7. Google Colab ç‰¹æœ‰ã®æ³¨æ„ç‚¹

### 7.1 ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†

```python
# ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå¯¾ç­–
import time
from IPython.display import Javascript

def keep_alive():
    """ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ç¶­æŒã™ã‚‹é–¢æ•°"""
    display(Javascript('''
        function KeepClicking(){
            console.log("Clicking");
            document.querySelector("colab-connect-button").click()
        }
        setInterval(KeepClicking, 60000)
    '''))

# é•·æ™‚é–“ã®å‡¦ç†å‰ã«å®Ÿè¡Œ
# keep_alive()
```

### 7.2 ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ã¨èª­ã¿è¾¼ã¿

```python
# ãƒ¢ãƒ‡ãƒ«ã‚’Google Driveã«ä¿å­˜ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
def save_model_to_drive():
    """ãƒ¢ãƒ‡ãƒ«ã‚’Google Driveã«ä¿å­˜"""
    
    try:
        from google.colab import drive
        drive.mount('/content/drive')
        
        # ä¿å­˜ãƒ‘ã‚¹
        save_path = '/content/drive/MyDrive/gpt2_model'
        
        # ãƒ¢ãƒ‡ãƒ«ã¨Tokenizerã‚’ä¿å­˜
        model.save_pretrained(save_path)
        tokenizer.save_pretrained(save_path)
        
        print(f"ãƒ¢ãƒ‡ãƒ«ã‚’ {save_path} ã«ä¿å­˜ã—ã¾ã—ãŸ")
        
    except Exception as e:
        print(f"ä¿å­˜ã«å¤±æ•—: {e}")

def load_model_from_drive():
    """Google Driveã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
    
    try:
        from google.colab import drive
        drive.mount('/content/drive')
        
        load_path = '/content/drive/MyDrive/gpt2_model'
        
        tokenizer_loaded = GPT2Tokenizer.from_pretrained(load_path)
        model_loaded = GPT2LMHeadModel.from_pretrained(load_path)
        
        print(f"ãƒ¢ãƒ‡ãƒ«ã‚’ {load_path} ã‹ã‚‰èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
        return tokenizer_loaded, model_loaded
        
    except Exception as e:
        print(f"èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
        return None, None
```

## 8. ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªãƒ‡ãƒ¢

### 8.1 ç°¡å˜ãªWebã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹

```python
# Gradioã‚’ä½¿ç”¨ã—ãŸWebã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
try:
    import gradio as gr
    
    def gradio_generate(prompt, max_length, temperature, top_p):
        """Gradioç”¨ã®ç”Ÿæˆé–¢æ•°"""
        result = generate_text(
            prompt, 
            max_length=int(max_length),
            temperature=float(temperature),
            top_p=float(top_p),
            num_return_sequences=1
        )
        return result[0] if result else ""
    
    # ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã®ä½œæˆ
    demo = gr.Interface(
        fn=gradio_generate,
        inputs=[
            gr.Textbox(label="ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ", placeholder="ã“ã“ã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„..."),
            gr.Slider(50, 200, value=100, label="æœ€å¤§é•·"),
            gr.Slider(0.1, 2.0, value=0.8, label="Temperature"),
            gr.Slider(0.1, 1.0, value=0.9, label="Top-p")
        ],
        outputs=gr.Textbox(label="ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ"),
        title="GPT-2 ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ",
        description="ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã—ã¾ã™"
    )
    
    # ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã®èµ·å‹•
    # demo.launch(share=True)  # ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆï¼ˆå¿…è¦ã«å¿œã˜ã¦å®Ÿè¡Œï¼‰
    
except ImportError:
    print("GradioãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã§ãã¾ã™:")
    print("!pip install gradio")
```

### 8.2 ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³é¢¨ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹

```python
def interactive_session():
    """å¯¾è©±çš„ãªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚»ãƒƒã‚·ãƒ§ãƒ³"""
    
    print("=== GPT-2 å¯¾è©±çš„ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚»ãƒƒã‚·ãƒ§ãƒ³ ===")
    print("çµ‚äº†ã™ã‚‹ã«ã¯ 'quit' ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    print("è¨­å®šã‚’å¤‰æ›´ã™ã‚‹ã«ã¯ 'config' ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    print()
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
    settings = {
        'max_length': 100,
        'temperature': 0.8,
        'top_p': 0.9
    }
    
    while True:
        prompt = input("ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ")
        
        if prompt.lower() == 'quit':
            print("ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
            break
        
        elif prompt.lower() == 'config':
            print(f"ç¾åœ¨ã®è¨­å®š: {settings}")
            try:
                new_max_length = input(f"æœ€å¤§é•· (ç¾åœ¨: {settings['max_length']}): ")
                if new_max_length:
                    settings['max_length'] = int(new_max_length)
                
                new_temperature = input(f"Temperature (ç¾åœ¨: {settings['temperature']}): ")
                if new_temperature:
                    settings['temperature'] = float(new_temperature)
                
                new_top_p = input(f"Top-p (ç¾åœ¨: {settings['top_p']}): ")
                if new_top_p:
                    settings['top_p'] = float(new_top_p)
                
                print("è¨­å®šã‚’æ›´æ–°ã—ã¾ã—ãŸã€‚")
            except ValueError:
                print("ç„¡åŠ¹ãªå€¤ã§ã™ã€‚è¨­å®šã¯å¤‰æ›´ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
            continue
        
        elif prompt.strip():
            try:
                print("\nç”Ÿæˆä¸­...")
                result = generate_text(
                    prompt,
                    max_length=settings['max_length'],
                    temperature=settings['temperature'],
                    top_p=settings['top_p'],
                    num_return_sequences=1
                )
                print(f"\nç”Ÿæˆçµæœ: {result[0]}\n")
                
            except Exception as e:
                print(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        
        else:
            print("ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

# å¯¾è©±ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®é–‹å§‹ï¼ˆå¿…è¦ã«å¿œã˜ã¦ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆè§£é™¤ï¼‰
# interactive_session()
```

## 9. ã¾ã¨ã‚ã¨æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

### 9.1 å­¦ç¿’ã—ãŸã“ã¨

ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã§ã¯ä»¥ä¸‹ã‚’å­¦ã³ã¾ã—ãŸï¼š

1. **ç’°å¢ƒè¨­å®š**: Google Colabã§ã®Hugging Face Transformersã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
2. **ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿**: äº‹å‰å­¦ç¿’æ¸ˆã¿GPT-2ãƒ¢ãƒ‡ãƒ«ã¨Tokenizerã®ä½¿ç”¨æ–¹æ³•
3. **åŸºæœ¬çš„ãªç”Ÿæˆ**: ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã®å®Ÿè£…
4. **é«˜åº¦ãªåˆ¶å¾¡**: æ§˜ã€…ãªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ‰‹æ³•ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´
5. **å®Ÿç”¨çš„ãªå¿œç”¨**: ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ç”Ÿæˆã€è³ªå•å¿œç­”ãªã©
6. **æœ€é©åŒ–**: ãƒ¡ãƒ¢ãƒªç®¡ç†ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®æ”¹å–„
7. **Colabç‰¹æœ‰ã®æ³¨æ„ç‚¹**: ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†ã¨ãƒ¢ãƒ‡ãƒ«ä¿å­˜

### 9.2 ã•ã‚‰ãªã‚‹å­¦ç¿’ãƒªã‚½ãƒ¼ã‚¹

```python
# å‚è€ƒãƒªãƒ³ã‚¯é›†
resources = {
    "å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ": [
        "Hugging Face Transformers: https://huggingface.co/docs/transformers",
        "GPT-2 Model Card: https://huggingface.co/gpt2"
    ],
    "ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«": [
        "Hugging Face Course: https://huggingface.co/course/",
        "PyTorch Tutorials: https://pytorch.org/tutorials/"
    ],
    "ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£": [
        "Hugging Face Forums: https://discuss.huggingface.co/",
        "Reddit r/MachineLearning: https://www.reddit.com/r/MachineLearning/"
    ]
}

for category, links in resources.items():
    print(f"{category}:")
    for link in links:
        print(f"  - {link}")
```

### 9.3 æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

- **ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°**: ç‹¬è‡ªã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®ãƒ¢ãƒ‡ãƒ«èª¿æ•´
- **ã‚ˆã‚Šå¤§ããªãƒ¢ãƒ‡ãƒ«**: GPT-3ã€GPT-4ã€Claude ãªã©ã®APIåˆ©ç”¨
- **ä»–ã®ã‚¿ã‚¹ã‚¯**: ç¿»è¨³ã€è¦ç´„ã€è³ªå•å¿œç­”ãªã©ã®ç‰¹åŒ–ã‚¿ã‚¹ã‚¯
- **æœ¬ç•ªç’°å¢ƒ**: FastAPIã€Flask ãªã©ã§ã®Webã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³åŒ–

```python
print("ğŸ‰ GPT-2 ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«å®Œäº†!")
print("Happy coding! ğŸš€")
```