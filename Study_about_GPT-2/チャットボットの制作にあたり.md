# GPT-2の事前学習モデルを使用して小さなチャットボットを制作する

## 概要

- Google Colabにて設計
- Hugging FaceからGPT-2の事前学習済みモデルをインポートして使用

---

## GPT-2について

- "Decoder"のみのTransformerモデルで、テキストの生成に特化している。
- 事前学習済みのため様々な文章生成は可能だが、会話に特化していない。
  - そのままではチャットボット向きではないため、調整・プロンプト設計が必要。
  - ➡ 今回のチャットボット制作の意図。
- トークナイザーは BPE (Byte Pair Encoding) を採用。

---

## トークナイザーについて

- 入力テキスト（ユーザーの入力したプロンプトなど）はトークナイザーによりトークン（単語・サブワード）に分解され、モデル（GPT-2）が理解できる数列に変換される。
- 出力はトークン列のため、トークン列 → 元の文章に戻すデコード処理が必要。

---

## モデル入力と出力の形状

- 入力は `バッチサイズ × シーケンス長` の整数テンソル。
- 生成時は次トークン予測を繰り返すループ処理が必要。

---

## テキスト生成のパラメータについて

以下のパラメータを調整して自然な会話を目指す：

1. `max_length`（最大生成長）
2. `temperature`（生成の多様性：低 → 保守的 / 高 → 多様性が高い）
3. `top_k` / `top_p`（確率分布の切り捨てによる生成の制御）

---

## 会話履歴の管理

- GPT-2は会話のコンテキストを覚えないため、ユーザーの入力（発言）と会話履歴を連結して入力に含める設計が基本。
- 入力長（GPT-2では最大512～1024トークン）が制限となるため注意。

---

## 動作確認の流れ

1. トークナイザーのロード  
2. GPT-2モデルのロード  
3. 入力テキストをトークナイズ  
4. モデルで生成実行  
5. 出力をデコードして表示

---

## 補足：BPE（Byte Pair Encoding）とサブワードについて

- **BPE（Byte Pair Encoding）**は、頻出する文字のペアを繰り返し結合していくことで、語彙を構築する手法です。これにより、未知語や複雑な単語も既存のサブワード（部分単語）に分解して扱えるようになります。
- GPT-2では、BPEを使って語彙を構築しており、これにより**語彙サイズを抑えつつ、柔軟な文章生成**が可能になります。
- **サブワード**とは、単語の一部（例：`un`, `break`, `able`など）であり、完全な単語ではないが意味を持つ単位です。これにより、未知の単語でも既知のサブワードに分解して処理できるため、モデルの汎用性が高まります。
